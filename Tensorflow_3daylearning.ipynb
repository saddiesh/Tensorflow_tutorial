{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephaniexia/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "# Create a constant operation\n",
    "# This op is added as a node to the default graph\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "\n",
    "# start a TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# run the op and get result\n",
    "print(sess.run(hello))\n",
    "##\n",
    "#https://stackoverflow.com/questions/6269765/what-does-the-b-character-do-in-front-of-a-string-literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.0, 2.0, 3.0]], [[7.0, 8.0, 9.0]]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 # a rank 0 tensor; this is a scalar with shape []\n",
    "[1. ,2., 3.] # a rank 1 tensor; this is a vector with shape [3]\n",
    "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]\n",
    "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: Tensor(\"Const_13:0\", shape=(), dtype=float32) \n",
      " node2: Tensor(\"Const_14:0\", shape=(), dtype=float32)\n",
      "node3:  Tensor(\"Add_8:0\", shape=(), dtype=float32)\n",
      "sess.run(node1, node2):  [3.0, 4.0]\n",
      "sess.run(node3):  7.0\n",
      "7.5\n",
      "[3. 7.]\n",
      "22.5\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "node3 = tf.add(node1, node2)\n",
    "sess = tf.Session()\n",
    "print(\"node1:\", node1,'\\n',\"node2:\", node2)\n",
    "print(\"node3: \", node3)\n",
    "print(\"sess.run(node1, node2): \", sess.run([node1, node2]))\n",
    "print(\"sess.run(node3): \", sess.run(node3))\n",
    "\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)\n",
    "\n",
    "print(sess.run(adder_node, feed_dict={a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, feed_dict={a: [1,3], b: [2, 4]}))\n",
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple, feed_dict={a: 3, b:4.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.355949 [0.56940526] [-1.1960759]\n",
      "20 4.355949 [0.56940526] [-1.1960759]\n",
      "40 4.355949 [0.56940526] [-1.1960759]\n",
      "60 4.355949 [0.56940526] [-1.1960759]\n",
      "80 4.355949 [0.56940526] [-1.1960759]\n",
      "100 4.355949 [0.56940526] [-1.1960759]\n",
      "120 4.355949 [0.56940526] [-1.1960759]\n",
      "140 4.355949 [0.56940526] [-1.1960759]\n",
      "160 4.355949 [0.56940526] [-1.1960759]\n",
      "180 4.355949 [0.56940526] [-1.1960759]\n",
      "200 4.355949 [0.56940526] [-1.1960759]\n",
      "220 4.355949 [0.56940526] [-1.1960759]\n",
      "240 4.355949 [0.56940526] [-1.1960759]\n",
      "260 4.355949 [0.56940526] [-1.1960759]\n",
      "280 4.355949 [0.56940526] [-1.1960759]\n",
      "300 4.355949 [0.56940526] [-1.1960759]\n",
      "320 4.355949 [0.56940526] [-1.1960759]\n",
      "340 4.355949 [0.56940526] [-1.1960759]\n",
      "360 4.355949 [0.56940526] [-1.1960759]\n",
      "380 4.355949 [0.56940526] [-1.1960759]\n",
      "400 4.355949 [0.56940526] [-1.1960759]\n",
      "420 4.355949 [0.56940526] [-1.1960759]\n",
      "440 4.355949 [0.56940526] [-1.1960759]\n",
      "460 4.355949 [0.56940526] [-1.1960759]\n",
      "480 4.355949 [0.56940526] [-1.1960759]\n",
      "500 4.355949 [0.56940526] [-1.1960759]\n",
      "520 4.355949 [0.56940526] [-1.1960759]\n",
      "540 4.355949 [0.56940526] [-1.1960759]\n",
      "560 4.355949 [0.56940526] [-1.1960759]\n",
      "580 4.355949 [0.56940526] [-1.1960759]\n",
      "600 4.355949 [0.56940526] [-1.1960759]\n",
      "620 4.355949 [0.56940526] [-1.1960759]\n",
      "640 4.355949 [0.56940526] [-1.1960759]\n",
      "660 4.355949 [0.56940526] [-1.1960759]\n",
      "680 4.355949 [0.56940526] [-1.1960759]\n",
      "700 4.355949 [0.56940526] [-1.1960759]\n",
      "720 4.355949 [0.56940526] [-1.1960759]\n",
      "740 4.355949 [0.56940526] [-1.1960759]\n",
      "760 4.355949 [0.56940526] [-1.1960759]\n",
      "780 4.355949 [0.56940526] [-1.1960759]\n",
      "800 4.355949 [0.56940526] [-1.1960759]\n",
      "820 4.355949 [0.56940526] [-1.1960759]\n",
      "840 4.355949 [0.56940526] [-1.1960759]\n",
      "860 4.355949 [0.56940526] [-1.1960759]\n",
      "880 4.355949 [0.56940526] [-1.1960759]\n",
      "900 4.355949 [0.56940526] [-1.1960759]\n",
      "920 4.355949 [0.56940526] [-1.1960759]\n",
      "940 4.355949 [0.56940526] [-1.1960759]\n",
      "960 4.355949 [0.56940526] [-1.1960759]\n",
      "980 4.355949 [0.56940526] [-1.1960759]\n",
      "1000 4.355949 [0.56940526] [-1.1960759]\n",
      "1020 4.355949 [0.56940526] [-1.1960759]\n",
      "1040 4.355949 [0.56940526] [-1.1960759]\n",
      "1060 4.355949 [0.56940526] [-1.1960759]\n",
      "1080 4.355949 [0.56940526] [-1.1960759]\n",
      "1100 4.355949 [0.56940526] [-1.1960759]\n",
      "1120 4.355949 [0.56940526] [-1.1960759]\n",
      "1140 4.355949 [0.56940526] [-1.1960759]\n",
      "1160 4.355949 [0.56940526] [-1.1960759]\n",
      "1180 4.355949 [0.56940526] [-1.1960759]\n",
      "1200 4.355949 [0.56940526] [-1.1960759]\n",
      "1220 4.355949 [0.56940526] [-1.1960759]\n",
      "1240 4.355949 [0.56940526] [-1.1960759]\n",
      "1260 4.355949 [0.56940526] [-1.1960759]\n",
      "1280 4.355949 [0.56940526] [-1.1960759]\n",
      "1300 4.355949 [0.56940526] [-1.1960759]\n",
      "1320 4.355949 [0.56940526] [-1.1960759]\n",
      "1340 4.355949 [0.56940526] [-1.1960759]\n",
      "1360 4.355949 [0.56940526] [-1.1960759]\n",
      "1380 4.355949 [0.56940526] [-1.1960759]\n",
      "1400 4.355949 [0.56940526] [-1.1960759]\n",
      "1420 4.355949 [0.56940526] [-1.1960759]\n",
      "1440 4.355949 [0.56940526] [-1.1960759]\n",
      "1460 4.355949 [0.56940526] [-1.1960759]\n",
      "1480 4.355949 [0.56940526] [-1.1960759]\n",
      "1500 4.355949 [0.56940526] [-1.1960759]\n",
      "1520 4.355949 [0.56940526] [-1.1960759]\n",
      "1540 4.355949 [0.56940526] [-1.1960759]\n",
      "1560 4.355949 [0.56940526] [-1.1960759]\n",
      "1580 4.355949 [0.56940526] [-1.1960759]\n",
      "1600 4.355949 [0.56940526] [-1.1960759]\n",
      "1620 4.355949 [0.56940526] [-1.1960759]\n",
      "1640 4.355949 [0.56940526] [-1.1960759]\n",
      "1660 4.355949 [0.56940526] [-1.1960759]\n",
      "1680 4.355949 [0.56940526] [-1.1960759]\n",
      "1700 4.355949 [0.56940526] [-1.1960759]\n",
      "1720 4.355949 [0.56940526] [-1.1960759]\n",
      "1740 4.355949 [0.56940526] [-1.1960759]\n",
      "1760 4.355949 [0.56940526] [-1.1960759]\n",
      "1780 4.355949 [0.56940526] [-1.1960759]\n",
      "1800 4.355949 [0.56940526] [-1.1960759]\n",
      "1820 4.355949 [0.56940526] [-1.1960759]\n",
      "1840 4.355949 [0.56940526] [-1.1960759]\n",
      "1860 4.355949 [0.56940526] [-1.1960759]\n",
      "1880 4.355949 [0.56940526] [-1.1960759]\n",
      "1900 4.355949 [0.56940526] [-1.1960759]\n",
      "1920 4.355949 [0.56940526] [-1.1960759]\n",
      "1940 4.355949 [0.56940526] [-1.1960759]\n",
      "1960 4.355949 [0.56940526] [-1.1960759]\n",
      "1980 4.355949 [0.56940526] [-1.1960759]\n",
      "2000 4.355949 [0.56940526] [-1.1960759]\n"
     ]
    }
   ],
   "source": [
    "# X and Y data\n",
    "#x_train = [1, 2, 3]\n",
    "#y_train = [1, 2, 3]\n",
    "x = [1, 2, 3]\n",
    "y = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = x_train * W + b\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "#train = optimizer.minimize(cost)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# Fit the line\n",
    "for step in range(2001):\n",
    "   sess.run(train)\n",
    "   if step % 20 == 0:\n",
    "       print(step, sess.run(cost), sess.run(W), sess.run(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.7938073 [0.9611542] [-1.421718]\n",
      "20 0.20484978 [1.4403884] [-1.1448897]\n",
      "40 0.1648995 [1.4651502] [-1.0710866]\n",
      "60 0.14957257 [1.4476185] [-1.0188456]\n",
      "80 0.13584256 [1.4269943] [-0.97078234]\n",
      "100 0.123374306 [1.4069663] [-0.9251419]\n",
      "120 0.112050585 [1.3878442] [-0.88166195]\n",
      "140 0.10176607 [1.3696173] [-0.840227]\n",
      "160 0.09242558 [1.3522466] [-0.8007393]\n",
      "180 0.08394242 [1.3356923] [-0.76310754]\n",
      "200 0.07623787 [1.319916] [-0.7272444]\n",
      "220 0.069240406 [1.304881] [-0.6930665]\n",
      "240 0.06288528 [1.290553] [-0.6604949]\n",
      "260 0.05711341 [1.2768981] [-0.62945414]\n",
      "280 0.05187132 [1.2638848] [-0.5998721]\n",
      "300 0.047110375 [1.2514831] [-0.5716803]\n",
      "320 0.042786386 [1.2396642] [-0.5448134]\n",
      "340 0.03885926 [1.228401] [-0.5192091]\n",
      "360 0.03529259 [1.2176669] [-0.4948081]\n",
      "380 0.032053314 [1.2074375] [-0.47155398]\n",
      "400 0.02911135 [1.1976887] [-0.44939277]\n",
      "420 0.026439384 [1.1883981] [-0.428273]\n",
      "440 0.02401269 [1.179544] [-0.40814576]\n",
      "460 0.021808721 [1.1711061] [-0.38896444]\n",
      "480 0.019807013 [1.163065] [-0.37068465]\n",
      "500 0.017989064 [1.1554013] [-0.35326383]\n",
      "520 0.016337939 [1.148098] [-0.3366617]\n",
      "540 0.014838375 [1.1411378] [-0.32083982]\n",
      "560 0.01347646 [1.134505] [-0.3057615]\n",
      "580 0.012239528 [1.1281838] [-0.29139188]\n",
      "600 0.011116152 [1.1221598] [-0.2776976]\n",
      "620 0.0100958375 [1.1164186] [-0.2646467]\n",
      "640 0.009169229 [1.1109474] [-0.25220937]\n",
      "660 0.008327636 [1.1057333] [-0.24035645]\n",
      "680 0.00756328 [1.1007639] [-0.22906056]\n",
      "700 0.006869089 [1.0960286] [-0.2182955]\n",
      "720 0.006238617 [1.0915157] [-0.20803644]\n",
      "740 0.005666025 [1.0872147] [-0.19825947]\n",
      "760 0.005145971 [1.083116] [-0.18894204]\n",
      "780 0.0046736603 [1.0792098] [-0.18006253]\n",
      "800 0.0042446884 [1.0754873] [-0.1716003]\n",
      "820 0.0038551018 [1.0719397] [-0.16353573]\n",
      "840 0.0035012588 [1.0685587] [-0.15585016]\n",
      "860 0.0031799048 [1.0653367] [-0.14852573]\n",
      "880 0.0028880413 [1.0622661] [-0.14154562]\n",
      "900 0.0026229601 [1.0593398] [-0.13489337]\n",
      "920 0.0023822042 [1.056551] [-0.12855381]\n",
      "940 0.0021635531 [1.0538933] [-0.12251221]\n",
      "960 0.00196498 [1.0513606] [-0.11675458]\n",
      "980 0.0017846278 [1.0489469] [-0.11126758]\n",
      "1000 0.0016208255 [1.0466465] [-0.10603839]\n",
      "1020 0.0014720582 [1.0444542] [-0.10105499]\n",
      "1040 0.0013369512 [1.0423652] [-0.09630579]\n",
      "1060 0.0012142444 [1.0403744] [-0.09177994]\n",
      "1080 0.0011027906 [1.0384766] [-0.08746663]\n",
      "1100 0.0010015736 [1.0366684] [-0.08335599]\n",
      "1120 0.0009096448 [1.0349451] [-0.07943855]\n",
      "1140 0.00082615274 [1.0333028] [-0.07570516]\n",
      "1160 0.0007503286 [1.0317377] [-0.07214729]\n",
      "1180 0.0006814624 [1.0302463] [-0.06875671]\n",
      "1200 0.00061891 [1.0288247] [-0.06552535]\n",
      "1220 0.0005621051 [1.0274701] [-0.06244593]\n",
      "1240 0.00051051256 [1.0261792] [-0.05951122]\n",
      "1260 0.00046365816 [1.0249488] [-0.05671446]\n",
      "1280 0.00042109986 [1.0237763] [-0.05404911]\n",
      "1300 0.00038245335 [1.0226591] [-0.05150906]\n",
      "1320 0.00034734933 [1.021594] [-0.04908839]\n",
      "1340 0.0003154726 [1.0205796] [-0.04678158]\n",
      "1360 0.00028652095 [1.0196123] [-0.04458323]\n",
      "1380 0.00026021796 [1.0186903] [-0.04248789]\n",
      "1400 0.00023633461 [1.017812] [-0.04049106]\n",
      "1420 0.0002146421 [1.0169749] [-0.03858809]\n",
      "1440 0.00019494012 [1.0161772] [-0.03677459]\n",
      "1460 0.00017704947 [1.015417] [-0.03504634]\n",
      "1480 0.00016080063 [1.0146924] [-0.03339932]\n",
      "1500 0.00014604088 [1.014002] [-0.03182969]\n",
      "1520 0.00013263653 [1.0133439] [-0.03033383]\n",
      "1540 0.00012046399 [1.0127168] [-0.02890827]\n",
      "1560 0.00010940584 [1.012119] [-0.02754966]\n",
      "1580 9.93651e-05 [1.0115496] [-0.02625491]\n",
      "1600 9.024423e-05 [1.0110068] [-0.02502104]\n",
      "1620 8.1961625e-05 [1.0104895] [-0.02384519]\n",
      "1640 7.443858e-05 [1.0099965] [-0.02272455]\n",
      "1660 6.760707e-05 [1.0095267] [-0.02165652]\n",
      "1680 6.140081e-05 [1.009079] [-0.02063874]\n",
      "1700 5.5765206e-05 [1.0086523] [-0.01966878]\n",
      "1720 5.0647923e-05 [1.0082457] [-0.01874444]\n",
      "1740 4.5997964e-05 [1.0078582] [-0.01786351]\n",
      "1760 4.1777206e-05 [1.007489] [-0.01702401]\n",
      "1780 3.7942827e-05 [1.007137] [-0.01622395]\n",
      "1800 3.446063e-05 [1.0068015] [-0.01546149]\n",
      "1820 3.1297433e-05 [1.0064819] [-0.01473484]\n",
      "1840 2.8424538e-05 [1.0061773] [-0.01404237]\n",
      "1860 2.581552e-05 [1.0058869] [-0.01338242]\n",
      "1880 2.3446046e-05 [1.0056103] [-0.01275349]\n",
      "1900 2.1293772e-05 [1.0053467] [-0.01215414]\n",
      "1920 1.9339946e-05 [1.0050954] [-0.01158294]\n",
      "1940 1.75643e-05 [1.0048559] [-0.01103857]\n",
      "1960 1.5952812e-05 [1.0046276] [-0.01051978]\n",
      "1980 1.4488539e-05 [1.0044103] [-0.0100254]\n",
      "2000 1.3158441e-05 [1.004203] [-0.00955426]\n",
      "[5.0114603]\n",
      "[2.5009532]\n",
      "[1.4967502 3.5051563]\n"
     ]
    }
   ],
   "source": [
    "# Now we can use X and Y in place of x_data and y_data\n",
    "# # placeholders for a tensor that will be always fed using feed_dict\n",
    "# See http://stackoverflow.com/questions/36693740/\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = X * W + b\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the line\n",
    "for step in range(2001):\n",
    "   cost_val, W_val, b_val, _ = sess.run([cost, W, b, train],feed_dict={X: [1, 2, 3], Y: [1, 2, 3]})\n",
    "   if step % 20 == 0:\n",
    "       print(step, cost_val, W_val, b_val)\n",
    "# Testing our model\n",
    "print(sess.run(hypothesis, feed_dict={X: [5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [2.5]}))\n",
    "print(sess.run(hypothesis, \n",
    "                   feed_dict={X: [1.5, 3.5]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 11.065117 [-0.3685079] [0.99743813]\n",
      "20 0.13301973 [0.6801707] [1.4305818]\n",
      "40 0.030900678 [0.788932] [1.4515779]\n",
      "60 0.027249223 [0.8077764] [1.4342813]\n",
      "80 0.0247408 [0.81766003] [1.4142455]\n",
      "100 0.0224699 [0.8263104] [1.3948128]\n",
      "120 0.020407533 [0.8344808] [1.3762616]\n",
      "140 0.018534428 [0.8422605] [1.3585789]\n",
      "160 0.0168333 [0.8496736] [1.3417271]\n",
      "180 0.015288271 [0.85673845] [1.3256671]\n",
      "200 0.013885029 [0.86347127] [1.3103619]\n",
      "220 0.012610574 [0.8698877] [1.2957759]\n",
      "240 0.011453128 [0.87600255] [1.2818753]\n",
      "260 0.010401908 [0.88183004] [1.2686282]\n",
      "280 0.009447188 [0.8873835] [1.2560037]\n",
      "300 0.008580092 [0.89267606] [1.2439725]\n",
      "320 0.0077925795 [0.8977198] [1.2325069]\n",
      "340 0.0070773605 [0.9025266] [1.22158]\n",
      "360 0.0064277537 [0.9071076] [1.2111664]\n",
      "380 0.005837807 [0.91147316] [1.2012424]\n",
      "400 0.0053019896 [0.91563356] [1.1917847]\n",
      "420 0.0048153456 [0.9195985] [1.1827717]\n",
      "440 0.004373379 [0.92337704] [1.1741822]\n",
      "460 0.0039719637 [0.92697793] [1.1659962]\n",
      "480 0.0036074158 [0.9304097] [1.158195]\n",
      "500 0.0032763118 [0.93368024] [1.1507604]\n",
      "520 0.002975587 [0.93679696] [1.1436753]\n",
      "540 0.0027024879 [0.9397673] [1.1369232]\n",
      "560 0.0024544422 [0.942598] [1.1304883]\n",
      "580 0.0022291734 [0.94529563] [1.1243559]\n",
      "600 0.0020245637 [0.9478666] [1.1185114]\n",
      "620 0.0018387331 [0.9503167] [1.1129417]\n",
      "640 0.0016699735 [0.9526516] [1.107634]\n",
      "660 0.0015166933 [0.9548769] [1.1025754]\n",
      "680 0.0013774751 [0.9569976] [1.0977546]\n",
      "700 0.0012510469 [0.9590185] [1.0931605]\n",
      "720 0.0011362215 [0.96094453] [1.0887822]\n",
      "740 0.0010319302 [0.96278] [1.0846099]\n",
      "760 0.00093721 [0.96452934] [1.080633]\n",
      "780 0.0008511906 [0.9661964] [1.0768436]\n",
      "800 0.00077306334 [0.967785] [1.0732323]\n",
      "820 0.0007021069 [0.9692989] [1.0697905]\n",
      "840 0.0006376671 [0.97074187] [1.0665107]\n",
      "860 0.0005791365 [0.9721168] [1.063385]\n",
      "880 0.00052597985 [0.97342724] [1.060406]\n",
      "900 0.00047770466 [0.9746761] [1.0575672]\n",
      "920 0.00043385776 [0.9758662] [1.0548617]\n",
      "940 0.00039403667 [0.9770004] [1.0522834]\n",
      "960 0.00035787327 [0.9780813] [1.0498263]\n",
      "980 0.00032502456 [0.97911143] [1.0474846]\n",
      "1000 0.00029519075 [0.98009324] [1.045253]\n",
      "1020 0.00026809808 [0.98102874] [1.0431262]\n",
      "1040 0.00024348963 [0.98192024] [1.0410994]\n",
      "1060 0.00022114161 [0.98277] [1.0391679]\n",
      "1080 0.00020084421 [0.98357975] [1.037327]\n",
      "1100 0.00018240763 [0.98435146] [1.0355726]\n",
      "1120 0.00016566423 [0.9850869] [1.0339009]\n",
      "1140 0.00015045897 [0.9857878] [1.0323076]\n",
      "1160 0.00013665068 [0.98645574] [1.0307894]\n",
      "1180 0.00012410955 [0.9870922] [1.0293424]\n",
      "1200 0.000112719084 [0.9876988] [1.0279635]\n",
      "1220 0.000102372935 [0.98827684] [1.0266494]\n",
      "1240 9.297576e-05 [0.9888278] [1.025397]\n",
      "1260 8.4442065e-05 [0.9893529] [1.0242034]\n",
      "1280 7.669259e-05 [0.9898531] [1.023066]\n",
      "1300 6.965435e-05 [0.99033016] [1.0219821]\n",
      "1320 6.325882e-05 [0.9907846] [1.0209489]\n",
      "1340 5.7455345e-05 [0.99121755] [1.0199646]\n",
      "1360 5.2181975e-05 [0.9916304] [1.0190262]\n",
      "1380 4.7391484e-05 [0.99202365] [1.018132]\n",
      "1400 4.3041462e-05 [0.9923986] [1.0172797]\n",
      "1420 3.9091265e-05 [0.9927559] [1.0164677]\n",
      "1440 3.550288e-05 [0.9930963] [1.0156938]\n",
      "1460 3.224429e-05 [0.9934207] [1.0149564]\n",
      "1480 2.9284967e-05 [0.9937299] [1.0142534]\n",
      "1500 2.6597228e-05 [0.9940246] [1.0135834]\n",
      "1520 2.415527e-05 [0.99430543] [1.012945]\n",
      "1540 2.1938607e-05 [0.99457306] [1.0123367]\n",
      "1560 1.9924695e-05 [0.99482816] [1.0117569]\n",
      "1580 1.809594e-05 [0.9950713] [1.0112042]\n",
      "1600 1.6434717e-05 [0.99530286] [1.0106777]\n",
      "1620 1.4926195e-05 [0.99552363] [1.0101758]\n",
      "1640 1.355639e-05 [0.995734] [1.0096977]\n",
      "1660 1.2312031e-05 [0.9959345] [1.0092418]\n",
      "1680 1.1182004e-05 [0.9961256] [1.0088077]\n",
      "1700 1.015551e-05 [0.9963076] [1.0083936]\n",
      "1720 9.224202e-06 [0.9964811] [1.0079993]\n",
      "1740 8.376833e-06 [0.99664646] [1.0076234]\n",
      "1760 7.6083397e-06 [0.99680406] [1.0072651]\n",
      "1780 6.9102207e-06 [0.9969542] [1.0069238]\n",
      "1800 6.275592e-06 [0.9970974] [1.0065984]\n",
      "1820 5.700384e-06 [0.99723375] [1.0062883]\n",
      "1840 5.1765473e-06 [0.9973638] [1.0059928]\n",
      "1860 4.701435e-06 [0.9974877] [1.0057111]\n",
      "1880 4.2701126e-06 [0.9976058] [1.0054426]\n",
      "1900 3.8779613e-06 [0.99771833] [1.0051868]\n",
      "1920 3.521873e-06 [0.99782556] [1.004943]\n",
      "1940 3.1986413e-06 [0.9979277] [1.0047108]\n",
      "1960 2.9050807e-06 [0.99802506] [1.0044894]\n",
      "1980 2.6385553e-06 [0.9981179] [1.0042784]\n",
      "2000 2.3965822e-06 [0.9982064] [1.0040774]\n",
      "[5.995109]\n",
      "[3.4995933]\n",
      "[2.5013871 4.4978   ]\n"
     ]
    }
   ],
   "source": [
    "#change X and Y\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = X * W + b\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Fit the line\n",
    "for step in range(2001):\n",
    "   cost_val, W_val, b_val, _ = sess.run([cost, W, b, train],feed_dict={X: [1, 2, 3], Y: [2, 3, 4]})\n",
    "   if step % 20 == 0:\n",
    "       print(step, cost_val, W_val, b_val)\n",
    "# Testing our model\n",
    "print(sess.run(hypothesis, feed_dict={X: [5]}))\n",
    "print(sess.run(hypothesis, feed_dict={X: [2.5]}))\n",
    "print(sess.run(hypothesis, \n",
    "                   feed_dict={X: [1.5, 3.5]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.53425497\n",
      "200 0.4319168\n",
      "400 0.41678366\n",
      "600 0.4026415\n",
      "800 0.38931838\n",
      "1000 0.37671185\n",
      "1200 0.3647547\n",
      "1400 0.35339823\n",
      "1600 0.34260356\n",
      "1800 0.3323373\n",
      "2000 0.32256955\n",
      "2200 0.31327263\n",
      "2400 0.30442047\n",
      "2600 0.29598823\n",
      "2800 0.2879526\n",
      "3000 0.28029132\n",
      "3200 0.2729833\n",
      "3400 0.2660086\n",
      "3600 0.25934842\n",
      "3800 0.25298485\n",
      "4000 0.24690108\n",
      "4200 0.2410816\n",
      "4400 0.23551144\n",
      "4600 0.23017669\n",
      "4800 0.2250642\n",
      "5000 0.2201618\n",
      "5200 0.21545804\n",
      "5400 0.21094203\n",
      "5600 0.20660377\n",
      "5800 0.20243375\n",
      "6000 0.19842301\n",
      "6200 0.19456331\n",
      "6400 0.19084685\n",
      "6600 0.18726617\n",
      "6800 0.18381451\n",
      "7000 0.18048541\n",
      "7200 0.1772728\n",
      "7400 0.17417093\n",
      "7600 0.17117445\n",
      "7800 0.16827835\n",
      "8000 0.16547787\n",
      "8200 0.16276848\n",
      "8400 0.16014607\n",
      "8600 0.15760659\n",
      "8800 0.15514636\n",
      "9000 0.15276174\n",
      "9200 0.15044956\n",
      "9400 0.14820658\n",
      "9600 0.14602973\n",
      "9800 0.14391635\n",
      "10000 0.14186364\n",
      "\n",
      "Hypothesis:  [[0.02759607]\n",
      " [0.15435266]\n",
      " [0.2894528 ]\n",
      " [0.78851515]\n",
      " [0.94398165]\n",
      " [0.9816581 ]] \n",
      "Correct (Y):  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]] \n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "#tf.cast(x, dtype, name=None)此函数是类型转换函数\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "   # Initialize TensorFlow variables\n",
    "   sess.run(tf.global_variables_initializer())\n",
    "\n",
    "   for step in range(10001):\n",
    "       cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})\n",
    "       if step % 200 == 0:\n",
    "           print(step, cost_val)\n",
    "\n",
    "   # Accuracy report\n",
    "   h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                      feed_dict={X: x_data, Y: y_data})\n",
    "   print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict=tf.cast(True, dtype=tf.float32)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(759, 8) (759, 1)\n",
      "0 0.9226387 [[ 1.1504499 ]\n",
      " [ 0.22871277]\n",
      " [ 2.2622318 ]\n",
      " [-0.5734556 ]\n",
      " [ 0.57770956]\n",
      " [ 2.6249194 ]\n",
      " [-0.5600849 ]\n",
      " [ 0.14174804]]\n",
      "200 0.8728538 [[ 0.9887621 ]\n",
      " [ 0.04453988]\n",
      " [ 2.1779435 ]\n",
      " [-0.6438753 ]\n",
      " [ 0.48335603]\n",
      " [ 2.5170007 ]\n",
      " [-0.5819274 ]\n",
      " [ 0.06869129]]\n",
      "400 0.8315248 [[ 0.8521471 ]\n",
      " [-0.13608786]\n",
      " [ 2.0928473 ]\n",
      " [-0.69878185]\n",
      " [ 0.40896076]\n",
      " [ 2.4138315 ]\n",
      " [-0.58272725]\n",
      " [ 0.02081293]]\n",
      "600 0.7951244 [[ 0.72970945]\n",
      " [-0.30942744]\n",
      " [ 2.0097995 ]\n",
      " [-0.74521714]\n",
      " [ 0.34611806]\n",
      " [ 2.3140469 ]\n",
      " [-0.5766077 ]\n",
      " [-0.01457623]]\n",
      "800 0.7627946 [[ 0.61681986]\n",
      " [-0.4739524 ]\n",
      " [ 1.9300094 ]\n",
      " [-0.7862866 ]\n",
      " [ 0.29106122]\n",
      " [ 2.2171159 ]\n",
      " [-0.5694703 ]\n",
      " [-0.0429705 ]]\n",
      "1000 0.73412424 [[ 0.51152074]\n",
      " [-0.62916774]\n",
      " [ 1.8539692 ]\n",
      " [-0.82339764]\n",
      " [ 0.24201404]\n",
      " [ 2.1228728 ]\n",
      " [-0.5636594 ]\n",
      " [-0.06678379]]\n",
      "1200 0.70876354 [[ 0.41295084]\n",
      " [-0.77508974]\n",
      " [ 1.7818307 ]\n",
      " [-0.8572369 ]\n",
      " [ 0.19803941]\n",
      " [ 2.0312874 ]\n",
      " [-0.5599788 ]\n",
      " [-0.08712628]]\n",
      "1400 0.6863728 [[ 0.32067198]\n",
      " [-0.9119986 ]\n",
      " [ 1.7135777 ]\n",
      " [-0.88819075]\n",
      " [ 0.15854697]\n",
      " [ 1.9423646 ]\n",
      " [-0.5585747 ]\n",
      " [-0.10456741]]\n",
      "1600 0.6666219 [[ 0.23438624]\n",
      " [-1.0403115 ]\n",
      " [ 1.649106  ]\n",
      " [-0.91652316]\n",
      " [ 0.12309086]\n",
      " [ 1.8561057 ]\n",
      " [-0.55932415]\n",
      " [-0.11945725]]\n",
      "1800 0.6492 [[ 0.15382825]\n",
      " [-1.1605148 ]\n",
      " [ 1.5882632 ]\n",
      " [-0.94244593]\n",
      " [ 0.09129019]\n",
      " [ 1.772492  ]\n",
      " [-0.562004  ]\n",
      " [-0.13205718]]\n",
      "2000 0.63382053 [[ 0.07872639]\n",
      " [-1.2731235 ]\n",
      " [ 1.5308694 ]\n",
      " [-0.9661504 ]\n",
      " [ 0.06280009]\n",
      " [ 1.6914841 ]\n",
      " [-0.5663641 ]\n",
      " [-0.14259073]]\n",
      "2200 0.6202246 [[ 0.008796  ]\n",
      " [-1.3786583 ]\n",
      " [ 1.4767327 ]\n",
      " [-0.98781127]\n",
      " [ 0.0373026 ]\n",
      " [ 1.6130228 ]\n",
      " [-0.5721601 ]\n",
      " [-0.15126143]]\n",
      "2400 0.6081819 [[-0.05625788]\n",
      " [-1.477629  ]\n",
      " [ 1.4256562 ]\n",
      " [-1.0075934 ]\n",
      " [ 0.01450429]\n",
      " [ 1.5370352 ]\n",
      " [-0.5791652 ]\n",
      " [-0.15825863]]\n",
      "2600 0.59748954 [[-0.11673449]\n",
      " [-1.5705222 ]\n",
      " [ 1.377444  ]\n",
      " [-1.0256506 ]\n",
      " [-0.00586439]\n",
      " [ 1.4634371 ]\n",
      " [-0.5871759 ]\n",
      " [-0.16375872]]\n",
      "2800 0.5879708 [[-0.17293134]\n",
      " [-1.6578004 ]\n",
      " [ 1.3319062 ]\n",
      " [-1.0421277 ]\n",
      " [-0.02404952]\n",
      " [ 1.3921381 ]\n",
      " [-0.5960126 ]\n",
      " [-0.16792497]]\n",
      "3000 0.5794722 [[-0.22513926]\n",
      " [-1.7398938 ]\n",
      " [ 1.2888613 ]\n",
      " [-1.0571575 ]\n",
      " [-0.04027457]\n",
      " [ 1.3230435 ]\n",
      " [-0.60551745]\n",
      " [-0.17090814]]\n",
      "3200 0.5718609 [[-0.27363816]\n",
      " [-1.8172016 ]\n",
      " [ 1.2481357 ]\n",
      " [-1.070863  ]\n",
      " [-0.05474149]\n",
      " [ 1.2560576 ]\n",
      " [-0.61555517]\n",
      " [-0.17284563]]\n",
      "3400 0.56502354 [[-0.31869477]\n",
      " [-1.8900892 ]\n",
      " [ 1.2095677 ]\n",
      " [-1.0833558 ]\n",
      " [-0.06763235]\n",
      " [ 1.1910864 ]\n",
      " [-0.62600774]\n",
      " [-0.17386253]]\n",
      "3600 0.5588614 [[-0.36056095]\n",
      " [-1.9588944 ]\n",
      " [ 1.1730065 ]\n",
      " [-1.0947393 ]\n",
      " [-0.0791108 ]\n",
      " [ 1.1280361 ]\n",
      " [-0.6367746 ]\n",
      " [-0.17407237]]\n",
      "3800 0.553291 [[-0.3994733 ]\n",
      " [-2.023926  ]\n",
      " [ 1.1383129 ]\n",
      " [-1.1051073 ]\n",
      " [-0.0893237 ]\n",
      " [ 1.066819  ]\n",
      " [-0.6477697 ]\n",
      " [-0.17357735]]\n",
      "4000 0.54823947 [[-0.43565217]\n",
      " [-2.0854647 ]\n",
      " [ 1.1053572 ]\n",
      " [-1.1145431 ]\n",
      " [-0.09840279]\n",
      " [ 1.0073491 ]\n",
      " [-0.65891916]\n",
      " [-0.17246933]]\n",
      "4200 0.5436448 [[-0.4693022 ]\n",
      " [-2.1437662 ]\n",
      " [ 1.0740191 ]\n",
      " [-1.1231247 ]\n",
      " [-0.10646611]\n",
      " [ 0.94954467]\n",
      " [-0.67016023]\n",
      " [-0.17083128]]\n",
      "4400 0.5394536 [[-0.5006137 ]\n",
      " [-2.199065  ]\n",
      " [ 1.0441904 ]\n",
      " [-1.1309223 ]\n",
      " [-0.1136194 ]\n",
      " [ 0.8933288 ]\n",
      " [-0.68143904]\n",
      " [-0.16873702]]\n",
      "4600 0.5356195 [[-0.5297618 ]\n",
      " [-2.251573  ]\n",
      " [ 1.01577   ]\n",
      " [-1.1379988 ]\n",
      " [-0.11995745]\n",
      " [ 0.8386287 ]\n",
      " [-0.6927102 ]\n",
      " [-0.16625267]]\n",
      "4800 0.5321025 [[-0.5569082 ]\n",
      " [-2.3014836 ]\n",
      " [ 0.9886648 ]\n",
      " [-1.1444118 ]\n",
      " [-0.12556502]\n",
      " [ 0.7853757 ]\n",
      " [-0.7039367 ]\n",
      " [-0.16343713]]\n",
      "5000 0.5288682 [[-0.58220184]\n",
      " [-2.3489757 ]\n",
      " [ 0.962789  ]\n",
      " [-1.1502136 ]\n",
      " [-0.13051814]\n",
      " [ 0.7335054 ]\n",
      " [-0.7150849 ]\n",
      " [-0.16034317]]\n",
      "5200 0.5258864 [[-0.6057803 ]\n",
      " [-2.394208  ]\n",
      " [ 0.93806624]\n",
      " [-1.1554509 ]\n",
      " [-0.13488543]\n",
      " [ 0.6829571 ]\n",
      " [-0.7261276 ]\n",
      " [-0.15701751]]\n",
      "5400 0.5231312 [[-0.6277697 ]\n",
      " [-2.4373293 ]\n",
      " [ 0.9144242 ]\n",
      " [-1.1601669 ]\n",
      " [-0.13872787]\n",
      " [ 0.6336741 ]\n",
      " [-0.7370417 ]\n",
      " [-0.15350181]]\n",
      "5600 0.5205794 [[-0.6482863 ]\n",
      " [-2.478475  ]\n",
      " [ 0.8917964 ]\n",
      " [-1.1644005 ]\n",
      " [-0.1421006 ]\n",
      " [ 0.585602  ]\n",
      " [-0.7478077 ]\n",
      " [-0.14983314]]\n",
      "5800 0.518211 [[-0.6674368 ]\n",
      " [-2.517769  ]\n",
      " [ 0.8701226 ]\n",
      " [-1.1681874 ]\n",
      " [-0.14505303]\n",
      " [ 0.5386922 ]\n",
      " [-0.7584099 ]\n",
      " [-0.14604424]]\n",
      "6000 0.51600856 [[-0.6853198 ]\n",
      " [-2.555325  ]\n",
      " [ 0.8493466 ]\n",
      " [-1.17156   ]\n",
      " [-0.14762962]\n",
      " [ 0.4928966 ]\n",
      " [-0.7688352 ]\n",
      " [-0.14216419]]\n",
      "6200 0.5139563 [[-0.70202583]\n",
      " [-2.5912485 ]\n",
      " [ 0.8294169 ]\n",
      " [-1.1745477 ]\n",
      " [-0.14987029]\n",
      " [ 0.44817096]\n",
      " [-0.77907276]\n",
      " [-0.13821869]]\n",
      "6400 0.5120408 [[-0.7176383 ]\n",
      " [-2.6256344 ]\n",
      " [ 0.8102862 ]\n",
      " [-1.1771771 ]\n",
      " [-0.15181097]\n",
      " [ 0.4044735 ]\n",
      " [-0.7891137 ]\n",
      " [-0.13423045]]\n",
      "6600 0.51024973 [[-0.7322343 ]\n",
      " [-2.6585717 ]\n",
      " [ 0.79191047]\n",
      " [-1.1794732 ]\n",
      " [-0.15348409]\n",
      " [ 0.36176506]\n",
      " [-0.7989514 ]\n",
      " [-0.13021937]]\n",
      "6800 0.5085725 [[-0.74588424]\n",
      " [-2.6901426 ]\n",
      " [ 0.7742486 ]\n",
      " [-1.1814586 ]\n",
      " [-0.1549189 ]\n",
      " [ 0.32000843]\n",
      " [-0.8085802 ]\n",
      " [-0.12620284]]\n",
      "7000 0.5069993 [[-0.7586537 ]\n",
      " [-2.7204232 ]\n",
      " [ 0.75726336]\n",
      " [-1.183154  ]\n",
      " [-0.15614155]\n",
      " [ 0.27916858]\n",
      " [-0.8179965 ]\n",
      " [-0.12219664]]\n",
      "7200 0.5055215 [[-0.77060276]\n",
      " [-2.7494833 ]\n",
      " [ 0.74091965]\n",
      " [-1.184578  ]\n",
      " [-0.15717585]\n",
      " [ 0.23921241]\n",
      " [-0.82719773]\n",
      " [-0.11821409]]\n",
      "7400 0.5041313 [[-0.7817877 ]\n",
      " [-2.777389  ]\n",
      " [ 0.72518504]\n",
      " [-1.1857483 ]\n",
      " [-0.15804318]\n",
      " [ 0.20010877]\n",
      " [-0.8361814 ]\n",
      " [-0.11426683]]\n",
      "7600 0.5028223 [[-0.79225975]\n",
      " [-2.8042011 ]\n",
      " [ 0.71002907]\n",
      " [-1.1866816 ]\n",
      " [-0.15876271]\n",
      " [ 0.161828  ]\n",
      " [-0.84494686]\n",
      " [-0.11036509]]\n",
      "7800 0.5015877 [[-0.8020664 ]\n",
      " [-2.8299747 ]\n",
      " [ 0.69542384]\n",
      " [-1.187392  ]\n",
      " [-0.15935192]\n",
      " [ 0.12434204]\n",
      " [-0.85349417]\n",
      " [-0.10651763]]\n",
      "8000 0.5004221 [[-0.8112524 ]\n",
      " [-2.8547628 ]\n",
      " [ 0.6813428 ]\n",
      " [-1.1878935 ]\n",
      " [-0.15982646]\n",
      " [ 0.08762418]\n",
      " [-0.8618238 ]\n",
      " [-0.10273204]]\n",
      "8200 0.49932054 [[-0.81985795]\n",
      " [-2.8786151 ]\n",
      " [ 0.6677618 ]\n",
      " [-1.1881989 ]\n",
      " [-0.16020057]\n",
      " [ 0.05164928]\n",
      " [-0.86993676]\n",
      " [-0.09901468]]\n",
      "8400 0.4982782 [[-0.827921  ]\n",
      " [-2.9015765 ]\n",
      " [ 0.6546578 ]\n",
      " [-1.1883202 ]\n",
      " [-0.16048694]\n",
      " [ 0.01639322]\n",
      " [-0.87783444]\n",
      " [-0.09537105]]\n",
      "8600 0.49729082 [[-0.83547676]\n",
      " [-2.92369   ]\n",
      " [ 0.642009  ]\n",
      " [-1.1882677 ]\n",
      " [-0.16069712]\n",
      " [-0.01816686]\n",
      " [-0.8855189 ]\n",
      " [-0.09180573]]\n",
      "8800 0.4963548 [[-0.84255785]\n",
      " [-2.9449966 ]\n",
      " [ 0.62979573]\n",
      " [-1.188052  ]\n",
      " [-0.16084155]\n",
      " [-0.05205277]\n",
      " [-0.8929925 ]\n",
      " [-0.08832249]]\n",
      "9000 0.49546656 [[-0.8491949 ]\n",
      " [-2.965533  ]\n",
      " [ 0.61799896]\n",
      " [-1.1876827 ]\n",
      " [-0.16092959]\n",
      " [-0.08528529]\n",
      " [-0.9002578 ]\n",
      " [-0.0849244 ]]\n",
      "9200 0.49462277 [[-0.85541564]\n",
      " [-2.9853349 ]\n",
      " [ 0.6066008 ]\n",
      " [-1.1871685 ]\n",
      " [-0.16096973]\n",
      " [-0.11788424]\n",
      " [-0.9073178 ]\n",
      " [-0.08161391]]\n",
      "9400 0.49382082 [[-0.86124676]\n",
      " [-3.0044346 ]\n",
      " [ 0.59558475]\n",
      " [-1.1865165 ]\n",
      " [-0.16096956]\n",
      " [-0.14986873]\n",
      " [-0.9141753 ]\n",
      " [-0.07839288]]\n",
      "9600 0.4930579 [[-0.8667123 ]\n",
      " [-3.022864  ]\n",
      " [ 0.5849351 ]\n",
      " [-1.1857361 ]\n",
      " [-0.16093595]\n",
      " [-0.1812568 ]\n",
      " [-0.92083365]\n",
      " [-0.07526275]]\n",
      "9800 0.49233133 [[-0.871835  ]\n",
      " [-3.040653  ]\n",
      " [ 0.57463706]\n",
      " [-1.1848333 ]\n",
      " [-0.1608751 ]\n",
      " [-0.21206546]\n",
      " [-0.9272964 ]\n",
      " [-0.07222448]]\n",
      "10000 0.49163896 [[-0.87663615]\n",
      " [-3.0578282 ]\n",
      " [ 0.56467664]\n",
      " [-1.1838149 ]\n",
      " [-0.16079248]\n",
      " [-0.24231151]\n",
      " [-0.93356687]\n",
      " [-0.06927861]]\n",
      "\n",
      "Hypothesis:  [[0.41464114]\n",
      " [0.92479753]\n",
      " [0.12266347]\n",
      " [0.93961275]\n",
      " [0.13973951]\n",
      " [0.7020489 ]\n",
      " [0.9232688 ]\n",
      " [0.559088  ]\n",
      " [0.2055341 ]\n",
      " [0.58350605]\n",
      " [0.7742604 ]\n",
      " [0.16474693]\n",
      " [0.20235339]\n",
      " [0.29173344]\n",
      " [0.6813884 ]\n",
      " [0.55801916]\n",
      " [0.70139307]\n",
      " [0.8242874 ]\n",
      " [0.82095736]\n",
      " [0.6609053 ]\n",
      " [0.71025753]\n",
      " [0.12058447]\n",
      " [0.6644037 ]\n",
      " [0.65442926]\n",
      " [0.38753006]\n",
      " [0.92552614]\n",
      " [0.4975903 ]\n",
      " [0.6909745 ]\n",
      " [0.77277136]\n",
      " [0.4072635 ]\n",
      " [0.9431263 ]\n",
      " [0.84739083]\n",
      " [0.58405674]\n",
      " [0.7516211 ]\n",
      " [0.34066066]\n",
      " [0.67538667]\n",
      " [0.85377073]\n",
      " [0.53649265]\n",
      " [0.40535548]\n",
      " [0.42670387]\n",
      " [0.8618046 ]\n",
      " [0.30026203]\n",
      " [0.29453173]\n",
      " [0.06011421]\n",
      " [0.46613228]\n",
      " [0.9396123 ]\n",
      " [0.76512176]\n",
      " [0.67744195]\n",
      " [0.9313162 ]\n",
      " [0.9038587 ]\n",
      " [0.9147769 ]\n",
      " [0.2561806 ]\n",
      " [0.31786746]\n",
      " [0.9625431 ]\n",
      " [0.19070977]\n",
      " [0.55555904]\n",
      " [0.1478731 ]\n",
      " [0.7266402 ]\n",
      " [0.86312914]\n",
      " [0.46958742]\n",
      " [0.933481  ]\n",
      " [0.58690006]\n",
      " [0.62552273]\n",
      " [0.86839527]\n",
      " [0.67205054]\n",
      " [0.7017981 ]\n",
      " [0.94316185]\n",
      " [0.6886063 ]\n",
      " [0.86261123]\n",
      " [0.5822865 ]\n",
      " [0.35708907]\n",
      " [0.7956095 ]\n",
      " [0.9369726 ]\n",
      " [0.9101211 ]\n",
      " [0.89282453]\n",
      " [0.8406854 ]\n",
      " [0.43583035]\n",
      " [0.85896295]\n",
      " [0.8630621 ]\n",
      " [0.91504526]\n",
      " [0.8705677 ]\n",
      " [0.7997846 ]\n",
      " [0.57980865]\n",
      " [0.81994796]\n",
      " [0.5293262 ]\n",
      " [0.8947665 ]\n",
      " [0.41017482]\n",
      " [0.905178  ]\n",
      " [0.89595646]\n",
      " [0.81324065]\n",
      " [0.87048537]\n",
      " [0.56749743]\n",
      " [0.6990652 ]\n",
      " [0.5892976 ]\n",
      " [0.9116752 ]\n",
      " [0.9628835 ]\n",
      " [0.84704685]\n",
      " [0.73093885]\n",
      " [0.24768499]\n",
      " [0.52417946]\n",
      " [0.5862378 ]\n",
      " [0.95956933]\n",
      " [0.7958375 ]\n",
      " [0.69840467]\n",
      " [0.9132388 ]\n",
      " [0.61312383]\n",
      " [0.9142644 ]\n",
      " [0.86626107]\n",
      " [0.45474598]\n",
      " [0.3548959 ]\n",
      " [0.9320235 ]\n",
      " [0.86735857]\n",
      " [0.32334247]\n",
      " [0.45595753]\n",
      " [0.626429  ]\n",
      " [0.7983638 ]\n",
      " [0.86471766]\n",
      " [0.9182611 ]\n",
      " [0.1730201 ]\n",
      " [0.7252881 ]\n",
      " [0.8616113 ]\n",
      " [0.5827646 ]\n",
      " [0.6238638 ]\n",
      " [0.83019817]\n",
      " [0.76026934]\n",
      " [0.805443  ]\n",
      " [0.8624249 ]\n",
      " [0.6000582 ]\n",
      " [0.48832494]\n",
      " [0.33249974]\n",
      " [0.37697682]\n",
      " [0.83982515]\n",
      " [0.9188498 ]\n",
      " [0.8322873 ]\n",
      " [0.77778906]\n",
      " [0.8201388 ]\n",
      " [0.42892998]\n",
      " [0.8403209 ]\n",
      " [0.65767246]\n",
      " [0.8202685 ]\n",
      " [0.85605395]\n",
      " [0.5938191 ]\n",
      " [0.5279816 ]\n",
      " [0.76272535]\n",
      " [0.9320436 ]\n",
      " [0.69910455]\n",
      " [0.44732884]\n",
      " [0.94291335]\n",
      " [0.62176245]\n",
      " [0.68238443]\n",
      " [0.2817456 ]\n",
      " [0.47072142]\n",
      " [0.14578423]\n",
      " [0.37920645]\n",
      " [0.8855973 ]\n",
      " [0.8353969 ]\n",
      " [0.94627374]\n",
      " [0.10307015]\n",
      " [0.5473864 ]\n",
      " [0.7872316 ]\n",
      " [0.71828145]\n",
      " [0.8795449 ]\n",
      " [0.4218133 ]\n",
      " [0.8184279 ]\n",
      " [0.626348  ]\n",
      " [0.5838534 ]\n",
      " [0.69255555]\n",
      " [0.89480424]\n",
      " [0.7726915 ]\n",
      " [0.6384267 ]\n",
      " [0.8911759 ]\n",
      " [0.8914466 ]\n",
      " [0.94845456]\n",
      " [0.16877735]\n",
      " [0.822182  ]\n",
      " [0.55980146]\n",
      " [0.4734587 ]\n",
      " [0.4465294 ]\n",
      " [0.8589624 ]\n",
      " [0.6729901 ]\n",
      " [0.9236107 ]\n",
      " [0.8801441 ]\n",
      " [0.5299865 ]\n",
      " [0.13929728]\n",
      " [0.16222136]\n",
      " [0.62690437]\n",
      " [0.6858891 ]\n",
      " [0.6208551 ]\n",
      " [0.755123  ]\n",
      " [0.55818486]\n",
      " [0.28123477]\n",
      " [0.27240247]\n",
      " [0.8872309 ]\n",
      " [0.42338884]\n",
      " [0.8071508 ]\n",
      " [0.86985606]\n",
      " [0.67238444]\n",
      " [0.6058465 ]\n",
      " [0.69444287]\n",
      " [0.6317077 ]\n",
      " [0.71255916]\n",
      " [0.93178904]\n",
      " [0.8020006 ]\n",
      " [0.795983  ]\n",
      " [0.13965872]\n",
      " [0.40679726]\n",
      " [0.9075392 ]\n",
      " [0.23658928]\n",
      " [0.93676615]\n",
      " [0.32927468]\n",
      " [0.32074097]\n",
      " [0.51407725]\n",
      " [0.73716915]\n",
      " [0.22109127]\n",
      " [0.7240203 ]\n",
      " [0.6807602 ]\n",
      " [0.8351656 ]\n",
      " [0.67799973]\n",
      " [0.13626403]\n",
      " [0.35797632]\n",
      " [0.6031517 ]\n",
      " [0.43123654]\n",
      " [0.9092752 ]\n",
      " [0.9487186 ]\n",
      " [0.7138751 ]\n",
      " [0.32994464]\n",
      " [0.03856736]\n",
      " [0.7437562 ]\n",
      " [0.45112315]\n",
      " [0.58908075]\n",
      " [0.94835126]\n",
      " [0.62265265]\n",
      " [0.9463237 ]\n",
      " [0.25143698]\n",
      " [0.22133411]\n",
      " [0.30983955]\n",
      " [0.65443313]\n",
      " [0.92328626]\n",
      " [0.8814592 ]\n",
      " [0.50739795]\n",
      " [0.57024807]\n",
      " [0.6238942 ]\n",
      " [0.16787206]\n",
      " [0.49754462]\n",
      " [0.2846353 ]\n",
      " [0.6033992 ]\n",
      " [0.9083519 ]\n",
      " [0.58354443]\n",
      " [0.6863161 ]\n",
      " [0.95231074]\n",
      " [0.84414786]\n",
      " [0.7713542 ]\n",
      " [0.775055  ]\n",
      " [0.72813237]\n",
      " [0.8705832 ]\n",
      " [0.29948273]\n",
      " [0.3807694 ]\n",
      " [0.42638814]\n",
      " [0.83566403]\n",
      " [0.73505294]\n",
      " [0.6355953 ]\n",
      " [0.83725864]\n",
      " [0.27895477]\n",
      " [0.51919675]\n",
      " [0.46065834]\n",
      " [0.52098197]\n",
      " [0.57543683]\n",
      " [0.8666736 ]\n",
      " [0.7046222 ]\n",
      " [0.9270157 ]\n",
      " [0.53730935]\n",
      " [0.77293164]\n",
      " [0.7955065 ]\n",
      " [0.78940624]\n",
      " [0.60547704]\n",
      " [0.85564023]\n",
      " [0.3422839 ]\n",
      " [0.6060759 ]\n",
      " [0.7447887 ]\n",
      " [0.33933225]\n",
      " [0.79587585]\n",
      " [0.35415152]\n",
      " [0.72922796]\n",
      " [0.9055718 ]\n",
      " [0.77555627]\n",
      " [0.89220876]\n",
      " [0.67906046]\n",
      " [0.56943595]\n",
      " [0.6064919 ]\n",
      " [0.2305239 ]\n",
      " [0.41739467]\n",
      " [0.59459955]\n",
      " [0.63954467]\n",
      " [0.70051974]\n",
      " [0.52309215]\n",
      " [0.14822458]\n",
      " [0.61158276]\n",
      " [0.91897076]\n",
      " [0.7132132 ]\n",
      " [0.51230794]\n",
      " [0.7975731 ]\n",
      " [0.37968412]\n",
      " [0.6796685 ]\n",
      " [0.48026368]\n",
      " [0.7051746 ]\n",
      " [0.8941223 ]\n",
      " [0.6827835 ]\n",
      " [0.6646813 ]\n",
      " [0.8390348 ]\n",
      " [0.6274402 ]\n",
      " [0.86686933]\n",
      " [0.93570775]\n",
      " [0.2473396 ]\n",
      " [0.79145205]\n",
      " [0.167908  ]\n",
      " [0.7549105 ]\n",
      " [0.8353487 ]\n",
      " [0.70048696]\n",
      " [0.30088535]\n",
      " [0.8372614 ]\n",
      " [0.6996533 ]\n",
      " [0.74127394]\n",
      " [0.15242371]\n",
      " [0.8886747 ]\n",
      " [0.8344758 ]\n",
      " [0.5403476 ]\n",
      " [0.94479704]\n",
      " [0.26204   ]\n",
      " [0.6297479 ]\n",
      " [0.93469656]\n",
      " [0.27488124]\n",
      " [0.41841036]\n",
      " [0.6726808 ]\n",
      " [0.31452838]\n",
      " [0.19871737]\n",
      " [0.8315118 ]\n",
      " [0.9119188 ]\n",
      " [0.8687881 ]\n",
      " [0.65478903]\n",
      " [0.6910154 ]\n",
      " [0.63587606]\n",
      " [0.68937004]\n",
      " [0.7281967 ]\n",
      " [0.9098071 ]\n",
      " [0.8313495 ]\n",
      " [0.83299893]\n",
      " [0.58085924]\n",
      " [0.9611285 ]\n",
      " [0.93696624]\n",
      " [0.8087625 ]\n",
      " [0.25146905]\n",
      " [0.6077034 ]\n",
      " [0.38924283]\n",
      " [0.75219584]\n",
      " [0.19858089]\n",
      " [0.19944735]\n",
      " [0.3584675 ]\n",
      " [0.83317614]\n",
      " [0.41152695]\n",
      " [0.6056348 ]\n",
      " [0.8233999 ]\n",
      " [0.58523345]\n",
      " [0.8300293 ]\n",
      " [0.9572198 ]\n",
      " [0.81982595]\n",
      " [0.09807751]\n",
      " [0.44450513]\n",
      " [0.85618067]\n",
      " [0.8367051 ]\n",
      " [0.61594427]\n",
      " [0.3188084 ]\n",
      " [0.88293415]\n",
      " [0.89581877]\n",
      " [0.38748914]\n",
      " [0.7377515 ]\n",
      " [0.8328087 ]\n",
      " [0.8126476 ]\n",
      " [0.8578374 ]\n",
      " [0.88223416]\n",
      " [0.84419876]\n",
      " [0.88508314]\n",
      " [0.71534157]\n",
      " [0.77842164]\n",
      " [0.5924527 ]\n",
      " [0.8359257 ]\n",
      " [0.8803926 ]\n",
      " [0.28698564]\n",
      " [0.79954356]\n",
      " [0.85401946]\n",
      " [0.30396682]\n",
      " [0.55661976]\n",
      " [0.80098915]\n",
      " [0.53690547]\n",
      " [0.8771122 ]\n",
      " [0.27187666]\n",
      " [0.81703144]\n",
      " [0.6267792 ]\n",
      " [0.9084545 ]\n",
      " [0.29753464]\n",
      " [0.70328873]\n",
      " [0.6839803 ]\n",
      " [0.6849558 ]\n",
      " [0.05987064]\n",
      " [0.25532717]\n",
      " [0.7573101 ]\n",
      " [0.8350094 ]\n",
      " [0.6007718 ]\n",
      " [0.7573946 ]\n",
      " [0.43918037]\n",
      " [0.42055243]\n",
      " [0.8549234 ]\n",
      " [0.5502124 ]\n",
      " [0.8755949 ]\n",
      " [0.7618938 ]\n",
      " [0.7737244 ]\n",
      " [0.9116447 ]\n",
      " [0.65849894]\n",
      " [0.8177036 ]\n",
      " [0.40579754]\n",
      " [0.2847953 ]\n",
      " [0.72355133]\n",
      " [0.3910428 ]\n",
      " [0.53032434]\n",
      " [0.9271633 ]\n",
      " [0.8433595 ]\n",
      " [0.92347866]\n",
      " [0.95935875]\n",
      " [0.6220624 ]\n",
      " [0.9048799 ]\n",
      " [0.3740808 ]\n",
      " [0.43760064]\n",
      " [0.4259106 ]\n",
      " [0.93640846]\n",
      " [0.6587158 ]\n",
      " [0.23894373]\n",
      " [0.9319439 ]\n",
      " [0.8062812 ]\n",
      " [0.52823555]\n",
      " [0.8238865 ]\n",
      " [0.02652608]\n",
      " [0.9180592 ]\n",
      " [0.77870333]\n",
      " [0.7154645 ]\n",
      " [0.7631881 ]\n",
      " [0.9567627 ]\n",
      " [0.5736458 ]\n",
      " [0.8135297 ]\n",
      " [0.59802026]\n",
      " [0.8577916 ]\n",
      " [0.16602407]\n",
      " [0.5078144 ]\n",
      " [0.90222144]\n",
      " [0.5243622 ]\n",
      " [0.66133446]\n",
      " [0.91733944]\n",
      " [0.8426524 ]\n",
      " [0.8951906 ]\n",
      " [0.48552603]\n",
      " [0.7018788 ]\n",
      " [0.9323482 ]\n",
      " [0.7399362 ]\n",
      " [0.5667247 ]\n",
      " [0.37704694]\n",
      " [0.56903446]\n",
      " [0.5192488 ]\n",
      " [0.67805505]\n",
      " [0.5249447 ]\n",
      " [0.72689253]\n",
      " [0.5934478 ]\n",
      " [0.78466976]\n",
      " [0.8063454 ]\n",
      " [0.65831697]\n",
      " [0.6801969 ]\n",
      " [0.5102433 ]\n",
      " [0.6560372 ]\n",
      " [0.92198634]\n",
      " [0.88173324]\n",
      " [0.27009848]\n",
      " [0.4661211 ]\n",
      " [0.55197984]\n",
      " [0.1519523 ]\n",
      " [0.8976942 ]\n",
      " [0.10321818]\n",
      " [0.9120517 ]\n",
      " [0.92080843]\n",
      " [0.84394133]\n",
      " [0.66909105]\n",
      " [0.8841091 ]\n",
      " [0.2778541 ]\n",
      " [0.71309894]\n",
      " [0.9470381 ]\n",
      " [0.20790604]\n",
      " [0.38602024]\n",
      " [0.88207024]\n",
      " [0.91011083]\n",
      " [0.70416796]\n",
      " [0.8087482 ]\n",
      " [0.86164576]\n",
      " [0.82519555]\n",
      " [0.3069554 ]\n",
      " [0.72672826]\n",
      " [0.88066536]\n",
      " [0.555215  ]\n",
      " [0.8020862 ]\n",
      " [0.62443244]\n",
      " [0.75092334]\n",
      " [0.82800853]\n",
      " [0.9096601 ]\n",
      " [0.5900076 ]\n",
      " [0.40376216]\n",
      " [0.74479115]\n",
      " [0.7731395 ]\n",
      " [0.96029806]\n",
      " [0.77737623]\n",
      " [0.6584617 ]\n",
      " [0.38247764]\n",
      " [0.6567907 ]\n",
      " [0.9140034 ]\n",
      " [0.92735887]\n",
      " [0.8902252 ]\n",
      " [0.66177326]\n",
      " [0.551178  ]\n",
      " [0.7863536 ]\n",
      " [0.5685947 ]\n",
      " [0.81934226]\n",
      " [0.75585383]\n",
      " [0.8837598 ]\n",
      " [0.5575131 ]\n",
      " [0.7149643 ]\n",
      " [0.84510195]\n",
      " [0.5129106 ]\n",
      " [0.5622208 ]\n",
      " [0.7004045 ]\n",
      " [0.73709357]\n",
      " [0.73809576]\n",
      " [0.94319487]\n",
      " [0.93179774]\n",
      " [0.21570954]\n",
      " [0.17749766]\n",
      " [0.7696089 ]\n",
      " [0.4873788 ]\n",
      " [0.28363034]\n",
      " [0.8432848 ]\n",
      " [0.90319085]\n",
      " [0.69196093]\n",
      " [0.92968416]\n",
      " [0.9320775 ]\n",
      " [0.7022508 ]\n",
      " [0.8743356 ]\n",
      " [0.65505326]\n",
      " [0.6665761 ]\n",
      " [0.7442139 ]\n",
      " [0.60944957]\n",
      " [0.12222191]\n",
      " [0.91491973]\n",
      " [0.85401946]\n",
      " [0.7228944 ]\n",
      " [0.8986048 ]\n",
      " [0.8892629 ]\n",
      " [0.8722662 ]\n",
      " [0.5524282 ]\n",
      " [0.68166965]\n",
      " [0.8878034 ]\n",
      " [0.7025847 ]\n",
      " [0.8120107 ]\n",
      " [0.91965157]\n",
      " [0.61902285]\n",
      " [0.72579604]\n",
      " [0.74097955]\n",
      " [0.6444809 ]\n",
      " [0.42662716]\n",
      " [0.04827124]\n",
      " [0.34278846]\n",
      " [0.79976666]\n",
      " [0.6613064 ]\n",
      " [0.71997565]\n",
      " [0.599226  ]\n",
      " [0.93081135]\n",
      " [0.40340027]\n",
      " [0.74131775]\n",
      " [0.28985518]\n",
      " [0.8360154 ]\n",
      " [0.4866884 ]\n",
      " [0.79526556]\n",
      " [0.5917098 ]\n",
      " [0.82903385]\n",
      " [0.55234176]\n",
      " [0.24707182]\n",
      " [0.8637549 ]\n",
      " [0.8972869 ]\n",
      " [0.3795663 ]\n",
      " [0.8679617 ]\n",
      " [0.9019298 ]\n",
      " [0.78165674]\n",
      " [0.8039619 ]\n",
      " [0.4328844 ]\n",
      " [0.2206258 ]\n",
      " [0.7192965 ]\n",
      " [0.1993491 ]\n",
      " [0.9212029 ]\n",
      " [0.40357387]\n",
      " [0.91330343]\n",
      " [0.8665591 ]\n",
      " [0.37716505]\n",
      " [0.2349092 ]\n",
      " [0.7467561 ]\n",
      " [0.45621136]\n",
      " [0.7945945 ]\n",
      " [0.71900886]\n",
      " [0.97260904]\n",
      " [0.58044165]\n",
      " [0.5734999 ]\n",
      " [0.8257959 ]\n",
      " [0.80989426]\n",
      " [0.09373142]\n",
      " [0.8170043 ]\n",
      " [0.7803982 ]\n",
      " [0.8670878 ]\n",
      " [0.52946466]\n",
      " [0.4478762 ]\n",
      " [0.62002003]\n",
      " [0.90122676]\n",
      " [0.543422  ]\n",
      " [0.77508265]\n",
      " [0.7482233 ]\n",
      " [0.8717674 ]\n",
      " [0.72919   ]\n",
      " [0.5281904 ]\n",
      " [0.76842594]\n",
      " [0.9141819 ]\n",
      " [0.759269  ]\n",
      " [0.94742894]\n",
      " [0.82082355]\n",
      " [0.6044635 ]\n",
      " [0.45035744]\n",
      " [0.7846852 ]\n",
      " [0.8298337 ]\n",
      " [0.5499635 ]\n",
      " [0.59830946]\n",
      " [0.18077543]\n",
      " [0.5200148 ]\n",
      " [0.78475964]\n",
      " [0.92785645]\n",
      " [0.8320147 ]\n",
      " [0.70882505]\n",
      " [0.6579492 ]\n",
      " [0.8972426 ]\n",
      " [0.5358725 ]\n",
      " [0.8844526 ]\n",
      " [0.60712785]\n",
      " [0.86936915]\n",
      " [0.26208898]\n",
      " [0.10296004]\n",
      " [0.34192348]\n",
      " [0.34173092]\n",
      " [0.6877265 ]\n",
      " [0.8321534 ]\n",
      " [0.6741603 ]\n",
      " [0.7324623 ]\n",
      " [0.7972054 ]\n",
      " [0.46492302]\n",
      " [0.35594884]\n",
      " [0.9057246 ]\n",
      " [0.93767023]\n",
      " [0.67992616]\n",
      " [0.71551347]\n",
      " [0.1355503 ]\n",
      " [0.32210568]\n",
      " [0.7242019 ]\n",
      " [0.66851056]\n",
      " [0.8822248 ]\n",
      " [0.9685006 ]\n",
      " [0.29520354]\n",
      " [0.7632535 ]\n",
      " [0.6060227 ]\n",
      " [0.45487845]\n",
      " [0.74222636]\n",
      " [0.6193909 ]\n",
      " [0.88616985]\n",
      " [0.6711859 ]\n",
      " [0.59083176]\n",
      " [0.58797354]\n",
      " [0.25236282]\n",
      " [0.7092081 ]\n",
      " [0.5020127 ]\n",
      " [0.8665267 ]\n",
      " [0.6414293 ]\n",
      " [0.5474733 ]\n",
      " [0.70673054]\n",
      " [0.8018991 ]\n",
      " [0.5433876 ]\n",
      " [0.79141647]\n",
      " [0.6475529 ]\n",
      " [0.40406877]\n",
      " [0.6096716 ]\n",
      " [0.88536763]\n",
      " [0.89658874]\n",
      " [0.53656995]\n",
      " [0.77887386]\n",
      " [0.26154506]\n",
      " [0.86397946]\n",
      " [0.6142223 ]\n",
      " [0.7379966 ]\n",
      " [0.45789576]\n",
      " [0.5928702 ]\n",
      " [0.821212  ]\n",
      " [0.12848738]\n",
      " [0.2720237 ]\n",
      " [0.80158526]\n",
      " [0.79157484]\n",
      " [0.84985334]\n",
      " [0.9362208 ]\n",
      " [0.8108753 ]\n",
      " [0.65255064]\n",
      " [0.79569143]\n",
      " [0.81317014]\n",
      " [0.7687582 ]\n",
      " [0.8305876 ]\n",
      " [0.54160154]\n",
      " [0.35921025]\n",
      " [0.8499481 ]\n",
      " [0.78716475]\n",
      " [0.61959726]\n",
      " [0.36593834]\n",
      " [0.8529433 ]\n",
      " [0.7669325 ]\n",
      " [0.8534808 ]\n",
      " [0.6550505 ]\n",
      " [0.91199285]\n",
      " [0.8916491 ]\n",
      " [0.8173365 ]\n",
      " [0.4864615 ]\n",
      " [0.8652356 ]\n",
      " [0.89933276]\n",
      " [0.35006917]\n",
      " [0.18636648]\n",
      " [0.7185674 ]\n",
      " [0.6025084 ]\n",
      " [0.860564  ]\n",
      " [0.33847478]\n",
      " [0.31232432]\n",
      " [0.37859342]\n",
      " [0.8016686 ]\n",
      " [0.8580362 ]\n",
      " [0.1964313 ]\n",
      " [0.39143637]\n",
      " [0.6509429 ]\n",
      " [0.53453153]\n",
      " [0.5222203 ]\n",
      " [0.820821  ]\n",
      " [0.17227018]\n",
      " [0.8950495 ]\n",
      " [0.25039473]\n",
      " [0.77306783]\n",
      " [0.6993338 ]\n",
      " [0.78556067]\n",
      " [0.8017816 ]\n",
      " [0.65541744]\n",
      " [0.90657043]] \n",
      "Correct (Y):  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "Accuracy:  0.7615283\n"
     ]
    }
   ],
   "source": [
    "# Lab 5 Logistic Regression Classifier\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "xy = np.loadtxt('/Users/stephaniexia/Documents/Code/tensorflow tutorial/data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([8, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis using sigmoid: tf.div(1., 1. + tf.exp(-tf.matmul(X, W)))\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost/loss function\n",
    "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) *tf.log(1 - hypothesis))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(10001):\n",
    "        cost_val,weights,_ = sess.run([cost,W, train], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, cost_val,weights)\n",
    "\n",
    "    # Accuracy report\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],\n",
    "                       feed_dict={X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect (Y): \", c, \"\\nAccuracy: \", a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.9433942\n",
      "200 0.4973698\n",
      "400 0.39985913\n",
      "600 0.32042462\n",
      "800 0.2544058\n",
      "1000 0.22787869\n",
      "1200 0.20667581\n",
      "1400 0.18904504\n",
      "1600 0.1741306\n",
      "1800 0.16134527\n",
      "2000 0.15026465\n",
      "--------------\n",
      "[[2.3747499e-03 9.9761933e-01 5.9930662e-06]] [1]\n",
      "--------------\n",
      "[[0.9067683  0.08872146 0.00451032]] [0]\n",
      "--------------\n",
      "[[7.5586923e-09 2.7310775e-04 9.9972683e-01]] [2]\n",
      "--------------\n",
      "[[2.3747478e-03 9.9761933e-01 5.9930721e-06]\n",
      " [9.0676832e-01 8.8721462e-02 4.5103119e-03]\n",
      " [7.5586923e-09 2.7310775e-04 9.9972683e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Lab 6 Softmax Classifier\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 200 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    # Testing & One-hot encoding\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
    "    print(a, sess.run(tf.argmax(a, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
    "    print(b, sess.run(tf.argmax(b, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
    "    print(c, sess.run(tf.argmax(c, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "\n",
    "    all = sess.run(hypothesis, feed_dict={\n",
    "                   X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
    "    print(all, sess.run(tf.argmax(all, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "batch_xs, batch_ys = mnist.train.next_batch(100)#arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 2.956114819\n",
      "Epoch: 0002 cost = 1.149938548\n",
      "Epoch: 0003 cost = 0.896135980\n",
      "Epoch: 0004 cost = 0.774797348\n",
      "Epoch: 0005 cost = 0.699681105\n",
      "Epoch: 0006 cost = 0.646229761\n",
      "Epoch: 0007 cost = 0.606484101\n",
      "Epoch: 0008 cost = 0.575170977\n",
      "Epoch: 0009 cost = 0.549424710\n",
      "Epoch: 0010 cost = 0.528247285\n",
      "Epoch: 0011 cost = 0.510796516\n",
      "Epoch: 0012 cost = 0.494784127\n",
      "Epoch: 0013 cost = 0.481255902\n",
      "Epoch: 0014 cost = 0.469455684\n",
      "Epoch: 0015 cost = 0.458053159\n",
      "Learning finished\n",
      "Accuracy:  0.8869\n",
      "Label:  [9]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "run() got multiple values for argument 'feed_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-b70a98e6cccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Label: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     print(\"Prediction: \", sess.run(hypothesis,\n\u001b[0;32m---> 65\u001b[0;31m         tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     plt.imshow(\n",
      "\u001b[0;31mTypeError\u001b[0m: run() got multiple values for argument 'feed_dict'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={\n",
    "                            X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={\n",
    "          X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(\n",
    "        tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "        cmap='Greys',\n",
    "        interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-50-c9cc6f3f386d>:38: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Epoch: 0001 cost = 131.294641930\n",
      "Epoch: 0002 cost = 37.392091682\n",
      "Epoch: 0003 cost = 23.555403930\n",
      "Epoch: 0004 cost = 16.285784615\n",
      "Epoch: 0005 cost = 11.712277943\n",
      "Epoch: 0006 cost = 8.726046864\n",
      "Epoch: 0007 cost = 6.495006505\n",
      "Epoch: 0008 cost = 4.737544796\n",
      "Epoch: 0009 cost = 3.568460204\n",
      "Epoch: 0010 cost = 2.664356393\n",
      "Epoch: 0011 cost = 1.933957644\n",
      "Epoch: 0012 cost = 1.549131294\n",
      "Epoch: 0013 cost = 1.156215109\n",
      "Epoch: 0014 cost = 0.878527616\n",
      "Epoch: 0015 cost = 0.751991418\n",
      "Learning Finished!\n",
      "Accuracy: 0.9486\n",
      "Label:  [6]\n",
      "Prediction:  [5]\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 MNIST and NN\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 7]\n",
      "2\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    print(tf.reduce_sum(([[1,2],[3,4]]),axis=-1).eval())\n",
    "    print(tf.reduce_mean([[1,2],[3,4]]).eval())\n",
    "    print(tf.reduce_mean(tf.reduce_sum(([[1,2],[3,4]]),axis=1)).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephaniexia/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[12.]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# https://www.tensorflow.org/api_guides/python/array_ops\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "sess = tf.InteractiveSession()\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "tf.matmul(matrix1, matrix2).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[0, 1, 2],\n",
    "     [2, 1, 0]]\n",
    "tf.argmax(x, axis=0).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([[[0, 1, 2], \n",
    "               [3, 4, 5]],\n",
    "              \n",
    "              [[6, 7, 8], \n",
    "               [9, 10, 11]]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr=tf.reshape(t, shape=[-1, 3]).eval()\n",
    "print(tr)\n",
    "tt=tf.reshape(tr,shape=[1,-1]).eval()\n",
    "tf.squeeze(tt).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2]],\n",
       "\n",
       "       [[ 3,  4,  5]],\n",
       "\n",
       "       [[ 6,  7,  8]],\n",
       "\n",
       "       [[ 9, 10, 11]]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(t, shape=[-1, 1, 3]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze([[0], [1], [2]]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]]\n",
      "\n",
      " [[ 6  7  8]\n",
      "  [ 9 10 11]]]\n"
     ]
    }
   ],
   "source": [
    "ts=tf.squeeze(t).eval()\n",
    "print(ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "2 5\n",
      "3 6\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip([1, 2, 3], [4, 5, 6]):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 7\n",
      "2 5 8\n",
      "3 6 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x, y, z in zip([1, 2, 3], [4, 5, 6], [7, 8, 9]):\n",
    "    print(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "array([[[ 0,  1,  2],\n",
      "        [ 3,  4,  5]],\n",
      "\n",
      "       [[ 6,  7,  8],\n",
      "        [ 9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "t = np.array([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [9, 10, 11]]])\n",
    "pp.pprint(t.shape)\n",
    "pp.pprint(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 2)\n",
      "array([[[ 0,  6],\n",
      "        [ 1,  7],\n",
      "        [ 2,  8]],\n",
      "\n",
      "       [[ 3,  9],\n",
      "        [ 4, 10],\n",
      "        [ 5, 11]]])\n"
     ]
    }
   ],
   "source": [
    "t1 = tf.transpose(t, [1, 2, 0])\n",
    "pp.pprint(sess.run(t1).shape)\n",
    "pp.pprint(sess.run(t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
