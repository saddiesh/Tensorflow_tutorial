{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搭建多层的 LSTM 网络实现 MNIST 分类\n",
    "\n",
    "通过本例，你可以了解到单层 LSTM 的实现，多层 LSTM 的实现。输入输出数据的格式。\n",
    "\n",
    "如果你已经熟悉 MNIST 数据集的话，应该知道每张手写数字都是一个 28 * 28 的图片，在 RNN 中，我们每个时间步输入一行，一共有 28 个时间步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-4de5c18900f7>:15: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/stephaniexia/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/stephaniexia/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/stephaniexia/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/stephaniexia/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/stephaniexia/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "(10000, 10)\n",
      "(784,)\n",
      "(55000, 10)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 不打印 warning \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# 设置GPU按需增长\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 用tensorflow 导入数据\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('../data/MNIST_data', one_hot=True) \n",
    "\n",
    "# 看看咱们样本的数量\n",
    "print(mnist.test.labels.shape)\n",
    "print(mnist.test.images[0].shape)\n",
    "print(mnist.train.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** 一、首先设置好模型用到的各个超参数 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3            #learning rate\n",
    "input_size = 28      # 每个时刻的输入特征是28维的，就是每个时刻输入一行，一行有 28 个像素\n",
    "timestep_size = 28   # 时序持续长度为28，即每做一次预测，需要先输入28行\n",
    "hidden_size = 256    # 隐含层的数量\n",
    "layer_num = 2        # LSTM layer 的层数\n",
    "class_num = 10       # 最后输出分类类别数量，如果是回归预测的话应该是 1\n",
    "cell_type = \"lstm\"   # lstm 或者 block_lstm\n",
    "\n",
    "X_input = tf.placeholder(tf.float32, [None, 784])\n",
    "y_input = tf.placeholder(tf.float32, [None, class_num])\n",
    "# 在训练和测试的时候，我们想用不同的 batch_size.所以采用占位符的方式\n",
    "batch_size = tf.placeholder(tf.int32, [])  # 注意类型必须为 tf.int32, batch_size = 128\n",
    "keep_prob = tf.placeholder(tf.float32, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** 二、开始搭建 LSTM 模型，其实普通 RNNs 模型也一样 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 28, 256)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8cb3839f2ae7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mh_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# # *************** 为了更好的理解 LSTM 工作原理，我们把上面 步骤6 中的函数自己来实现 ***************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# 把784个点的字符信息还原成 28 * 28 的图片\n",
    "# 下面几个步骤是实现 RNN / LSTM 的关键\n",
    "\n",
    "# **步骤1：RNN 的输入shape = (batch_size, timestep_size, input_size) \n",
    "X = tf.reshape(X_input, [-1, 28, 28])\n",
    "\n",
    "# ** 步骤2：创建 lstm 结构\n",
    "def lstm_cell(cell_type, num_nodes, keep_prob):\n",
    "    assert(cell_type in [\"lstm\", \"block_lstm\"], \"Wrong cell type.\")\n",
    "    if cell_type == \"lstm\":\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_nodes)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_nodes)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    return cell\n",
    "\n",
    "mlstm_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(cell_type, hidden_size, keep_prob) for _ in range(layer_num)], state_is_tuple = True)\n",
    "\n",
    "# **步骤3：用全零来初始化state\n",
    "init_state = mlstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "# **步骤4：方法一，调用 dynamic_rnn() 来让我们构建好的网络运行起来\n",
    "# ** 当 time_major==False 时， outputs.shape = [batch_size, timestep_size, hidden_size] \n",
    "# ** 所以，可以取 h_state = outputs[:, -1, :] 作为最后输出\n",
    "# ** state.shape = [layer_num, 2, batch_size, hidden_size], \n",
    "# ** 或者，可以取 h_state = state[-1][1] 作为最后输出\n",
    "# ** 最后输出维度是 [batch_size, hidden_size]\n",
    "outputs, state = tf.nn.dynamic_rnn(mlstm_cell, inputs=X, initial_state=init_state, time_major=False)\n",
    "h_out = outputs \n",
    "h_state = state\n",
    "print(h_out.shape)\n",
    "print(h_state.shape)\n",
    "\n",
    "# # *************** 为了更好的理解 LSTM 工作原理，我们把上面 步骤6 中的函数自己来实现 ***************\n",
    "# # 通过查看文档你会发现， RNNCell 都提供了一个 __call__()函数，我们可以用它来展开实现LSTM按时间步迭代。\n",
    "# # **步骤4：方法二，按时间步展开计算\n",
    "'''\n",
    "outputs = list()\n",
    "state = init_state\n",
    "with tf.variable_scope('RNN'):\n",
    "    for timestep in range(timestep_size):\n",
    "        (cell_output, state) = mlstm_cell(X[:, timestep, :],state)\n",
    "        outputs.append(cell_output)\n",
    "h_state = outputs[-1]\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** 三、最后设置 loss function 和 优化器，展开训练并完成测试 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500, train cost=0.014780, acc=0.950000; test cost=0.012916, acc=0.961100; pass 250.78204488754272s\n",
      "step 1000, train cost=0.004835, acc=0.990000; test cost=0.008975, acc=0.971900; pass 249.6948959827423s\n",
      "step 1500, train cost=0.002485, acc=1.000000; test cost=0.007063, acc=0.978900; pass 246.9551281929016s\n",
      "step 2000, train cost=0.003267, acc=0.980000; test cost=0.006719, acc=0.979900; pass 226.68185091018677s\n",
      "step 2500, train cost=0.012967, acc=0.970000; test cost=0.006056, acc=0.982500; pass 221.91320896148682s\n",
      "step 3000, train cost=0.000624, acc=1.000000; test cost=0.005193, acc=0.985000; pass 221.6401240825653s\n",
      "step 3500, train cost=0.001973, acc=0.990000; test cost=0.005221, acc=0.985800; pass 225.04665970802307s\n",
      "step 4000, train cost=0.002755, acc=0.990000; test cost=0.003945, acc=0.988200; pass 218.09171605110168s\n",
      "step 4500, train cost=0.000362, acc=1.000000; test cost=0.004136, acc=0.988900; pass 220.79458689689636s\n",
      "step 5000, train cost=0.000739, acc=1.000000; test cost=0.003923, acc=0.988300; pass 229.84524703025818s\n"
     ]
    }
   ],
   "source": [
    "############################################################################\n",
    "# 以下部分其实和之前写的多层 CNNs 来实现 MNIST 分类是一样的。\n",
    "# 只是在测试的时候也要设置一样的 batch_size.\n",
    "\n",
    "# 上面 LSTM 部分的输出会是一个 [hidden_size] 的tensor，我们要分类的话，还需要接一个 softmax 层\n",
    "# 首先定义 softmax 的连接权重矩阵和偏置\n",
    "\n",
    "import time\n",
    "\n",
    "# 开始训练和测试\n",
    "W = tf.Variable(tf.truncated_normal([hidden_size, class_num], stddev=0.1), dtype=tf.float32)\n",
    "bias = tf.Variable(tf.constant(0.1,shape=[class_num]), dtype=tf.float32)\n",
    "y_pre = tf.nn.softmax(tf.matmul(h_state, W) + bias)\n",
    "\n",
    "\n",
    "# 损失和评估函数\n",
    "cross_entropy = -tf.reduce_mean(y_input * tf.log(y_pre))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(y_input,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "time0 = time.time()\n",
    "for i in range(5000):\n",
    "    _batch_size=100\n",
    "    X_batch, y_batch = mnist.train.next_batch(batch_size=_batch_size)\n",
    "    cost, acc,  _ = sess.run([cross_entropy, accuracy, train_op], feed_dict={X_input: X_batch, y_input: y_batch, keep_prob: 0.7, batch_size: _batch_size})\n",
    "    if (i+1) % 500 == 0:\n",
    "        # 分 100 个batch 迭代\n",
    "        test_acc = 0.0\n",
    "        test_cost = 0.0\n",
    "        N = 100\n",
    "        for j in range(N):\n",
    "            X_batch, y_batch = mnist.test.next_batch(batch_size=_batch_size)\n",
    "            _cost, _acc = sess.run([cross_entropy, accuracy], feed_dict={X_input: X_batch, y_input: y_batch, keep_prob: 1.0, batch_size: _batch_size})\n",
    "            test_acc += _acc\n",
    "            test_cost += _cost\n",
    "        print(\"step {}, train cost={:.6f}, acc={:.6f}; test cost={:.6f}, acc={:.6f}; pass {}s\".format(i+1, cost, acc, test_cost/N, test_acc/N, time.time() - time0))\n",
    "        time0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们实现了一个两层的 lstm 网络实现了 MNIST 手写数字的识别，从结果来看，相当不错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、可视化看看 LSTM 的是怎么做分类的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "毕竟 LSTM 更多的是用来做时序相关的问题，要么是文本，要么是序列预测之类的，所以很难像 CNNs 一样非常直观地看到每一层中特征的变化。在这里，我想通过可视化的方式来帮助大家理解 LSTM 是怎么样一步一步地把图片正确的给分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 784) (5, 10)\n",
      "_outputs.shape = (28, 5, 256)\n",
      "arr_state.shape = (2, 2, 5, 256)\n"
     ]
    }
   ],
   "source": [
    "# 手写的结果 shape\n",
    "_batch_size = 5\n",
    "X_batch, y_batch = mnist.test.next_batch(_batch_size)\n",
    "print(X_batch.shape, y_batch.shape)\n",
    "_outputs, _state = np.array(sess.run([outputs, state],feed_dict={\n",
    "            X_input: X_batch, y_input: y_batch, keep_prob: 1.0, batch_size: _batch_size}))\n",
    "print('_outputs.shape =', np.asarray(_outputs).shape)\n",
    "print('arr_state.shape =', np.asarray(_state).shape)\n",
    "# 可见 outputs.shape = [ batch_size, timestep_size, hidden_size]\n",
    "# state.shape = [layer_num, 2, batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看下面我找了一个字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(mnist.train.labels[10:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先来看看这个字符样子,上半部分还挺像 2 来的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c43df99b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X3 = mnist.train.images[10]\n",
    "img3 = X3.reshape([28, 28])\n",
    "plt.imshow(img3, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看看在分类的时候，一行一行地输入，分为各个类别的概率会是什么样子的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 1, 256)\n",
      "(1, 256)\n"
     ]
    }
   ],
   "source": [
    "X3.shape = [-1, 784]\n",
    "y_batch = mnist.train.labels[10]\n",
    "y_batch.shape = [-1, class_num]\n",
    "\n",
    "X3_outputs = np.array(sess.run(outputs, feed_dict={\n",
    "            X_input: X3, y_input: y_batch, keep_prob: 1.0, batch_size: 1}))\n",
    "print(X3_outputs.shape)\n",
    "#X3_outputs.shape = [28, hidden_size]\n",
    "print(X3_outputs[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 28 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h_W, h_bias = sess.run([W, bias], feed_dict={\n",
    "            X_input:X3, y_input: y_batch, keep_prob: 1.0, batch_size: 1})\n",
    "h_bias = h_bias.reshape([-1, 10])\n",
    "\n",
    "bar_index = range(class_num)\n",
    "for i in range(X3_outputs.shape[0]):\n",
    "    plt.subplot(7, 4, i+1)\n",
    "    X3_h_shate = X3_outputs[i, :].reshape([-1, hidden_size])\n",
    "    pro = sess.run(tf.nn.softmax(tf.matmul(X3_h_shate, h_W) + h_bias))\n",
    "    plt.bar(bar_index, pro[0], width=0.2 , align='center')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的图中，为了更清楚地看到线条的变化，我把坐标都去了，每一行显示了 4 个图，共有 7 行，表示了一行一行读取过程中，模型对字符的识别。可以看到，在只看到前面的几行像素时，模型根本认不出来是什么字符，随着看到的像素越来越多，最后就基本确定了这个字符的正确分类."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
