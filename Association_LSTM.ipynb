{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephaniexia/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "random.seed(567)\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 1\n",
    "batch_size = 3\n",
    "num_boxes=5\n",
    "#display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "input_size = num_boxes*1029\n",
    "timesteps_size = 30 # timesteps\n",
    "hidden_nodes1 = 150 # hidden layer num of features\n",
    "hidden_nodes2 = 300\n",
    "n_classes = 2\n",
    "dim_bbox=num_boxes*4\n",
    "dim_class_logits=num_boxes*n_classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.shape: (3, 30, 5155)\n",
      "x[0].shape: (30, 5155)\n",
      "x[0][0].shape: (5155,)\n",
      "[[1 0 0 1 0]\n",
      " [1 1 0 0 1]\n",
      " [0 1 0 1 1]]\n",
      "<tf.Variable 'Variable_1:0' shape=(3, 5) dtype=int64_ref>\n"
     ]
    }
   ],
   "source": [
    "#num_boxes=5\n",
    "#from numpy import random\n",
    "#random.seed(567)\n",
    "x_data=random.random((3,30,num_boxes*1029)).astype('float32')\n",
    "print('x_data.shape:',x_data.shape)\n",
    "print('x[0].shape:',x_data[0].shape)\n",
    "print('x[0][0].shape:',x_data[0][0].shape)\n",
    "#X = tf.placeholder(\"float32\", [None, timesteps_size, input_size])\n",
    "X=x_data\n",
    "X=tf.Variable(x_data)\n",
    "\n",
    "# Tensorflow LSTM cell requires 2x n_hidden length (state & cell)\n",
    "#Y = tf.placeholder(\"float32\", [None, dim_gt])\n",
    "#Y=y_data\n",
    "\n",
    "#print('x[0][0][0].shape:',x_data[0][0][0].shape,x_data[0][0][0])\n",
    "#y_data=random.random((3,num_boxes*7)).astype('float32')\n",
    "class_ids=random.randint(0,n_classes,size=(batch_size,num_boxes))\n",
    "print(class_ids)\n",
    "Y_class_ids=tf.Variable(class_ids)\n",
    "print(Y_class_ids)\n",
    "gt_boxes=random.random(([batch_size,num_boxes,4])).astype('float32')\n",
    "#print(gt_boxes)\n",
    "Y_gt_boxes=tf.Variable(gt_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: pad zeros for detections:\n",
    "#x_feature:1024\n",
    "#x_box:4\n",
    "#x_class:num_classes-1 ????\n",
    "input_feature=['None*num_obj*1024']\n",
    "X_det=np.zeros((len(input_feature),num_boxes,1029))\n",
    "X_det=np.zeros((batch_size,timesteps_size,num_boxes,1029))\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "def generate_distance_matrix(pic1,pic2,features1,features2):\n",
    "    pic1=np.array(pic1)\n",
    "    pic2=np.array(pic2)\n",
    "    pic1_feature=features1\n",
    "    pic2_feature=features2\n",
    "    #pad 0, keep same number of boxes in two pictures\n",
    "    for pad_num in range(abs(len(pic1)-len(pic2))):\n",
    "        if len(pic1)<len(pic2):\n",
    "            pic1=np.row_stack((pic1,np.zeros(4)))\n",
    "            pic1_feature=np.row_stack((features1,np.zeros(1024)))\n",
    "        if len(pic2)<len(pic1):\n",
    "            pic2= np.row_stack((pic2,np.zeros(4)))\n",
    "            pic2_feature=np.row_stack((features2,np.zeros(1024)))\n",
    "    #initialize a distance matrix\n",
    "    distance_matrix=np.zeros((len(pic1),len(pic2))) #len(pic1)=len(pic2)\n",
    "    #calculate distance matrix\n",
    "    for i in range(len(pic1)):\n",
    "        box_i=pic1[i]\n",
    "        #find center of the box_i\n",
    "        cent_i =np.array([(box_i[1] + (box_i[3] - box_i[1]) / 2), (box_i[0] + (box_i[2] - box_i[0]) / 2)])\n",
    "        for j in range(len(pic2)):\n",
    "            box_j=pic2[j]\n",
    "            cent_j = np.array([(box_j[1] + (box_j[3] - box_j[1]) / 2), (box_j[0] + (box_j[2] - box_j[0]) / 2)])\n",
    "            #calculate Euclidean distance\n",
    "            distance_matrix[i,j]=np.sqrt(np.sum(np.square(cent_i-cent_j)))\n",
    "    return distance_matrix,pic1_feature,pic2_feature\n",
    "\n",
    "def box_pair_assignment(pic1,pic2,features1,features2):\n",
    "    distance_matrix,pic1_feature,pic2_feature=generate_distance_matrix(pic1, pic2,features1,features2)\n",
    "    #assign picture into pairs\n",
    "    pic1_ind,pic2_ind=linear_sum_assignment(distance_matrix)\n",
    "    #pairs=np.array((pic1_ind,pic2_ind))\n",
    "    return pic1_ind,pic2_ind,pic1_feature,pic2_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(h_out)\n",
    "def fc_bbox(input):\n",
    "    w1 = tf.get_variable('weight1', shape=[hidden_nodes2, dim_bbox], initializer=tf.truncated_normal_initializer())\n",
    "    b1 = tf.get_variable('bias1', [dim_bbox], initializer=tf.zeros_initializer())\n",
    "    h_fc_bbox = tf.nn.relu(tf.nn.xw_plus_b(input, w1, b1), name='relu')\n",
    "    print('fc_bbox completed')\n",
    "    return h_fc_bbox\n",
    "\n",
    "def fc_class_logits(input):\n",
    "    w1 = tf.get_variable('weight1', shape=[hidden_nodes2, dim_class_logits], initializer=tf.truncated_normal_initializer())\n",
    "    b1 = tf.get_variable('bias1', [dim_class_logits], initializer=tf.zeros_initializer())\n",
    "    h_fc_logits = tf.nn.relu(tf.nn.xw_plus_b(input, w1, b1), name='relu')\n",
    "    print('fc_class_logits completed')\n",
    "    return h_fc_logits\n",
    "\n",
    "def fc_2(input):\n",
    "    w2 = tf.get_variable('weight2', shape=[hidden_nodes2,50], initializer=tf.truncated_normal_initializer())\n",
    "    b2 = tf.get_variable('bias2', [50], initializer=tf.zeros_initializer())\n",
    "    h_fc_2 = tf.nn.relu(tf.nn.xw_plus_b(input, w2, b2), name='relu')\n",
    "    print('fc_2 completed')\n",
    "    return h_fc_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_ids=random.randint(0,n_classes,size=(batch_size,num_boxes))\n",
    "#print(class_ids)\n",
    "#Y_class_ids=tf.Variable(class_ids)\n",
    "#print(Y_class_ids)\n",
    "def class_loss_graph(h_fc_logits,Y_class_ids):\n",
    "    print('start computing class_loss_graph')\n",
    "    h_fc_logits=tf.reshape(h_fc_logits[:,-1,:,:],(batch_size,num_boxes,n_classes))\n",
    "    class_loss= tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y_class_ids, logits=h_fc_logits)\n",
    "    #print(sess.run(class_loss))\n",
    "    return class_loss\n",
    "#sess=tf.Session()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "#class_loss=tf.reduce_mean(class_loss_graph(h_fc_logits,Y_class_ids))\n",
    "#print(sess.run(class_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#gt_boxes=random.random(([batch_size,num_boxes,4])).astype('float32')\n",
    "#print(gt_boxes)\n",
    "#Y_gt_boxes=tf.Variable(gt_boxes)\n",
    "def smooth_l1_loss(y_true, y_pred):\n",
    "    diff = tf.math.abs(y_true - y_pred)\n",
    "    less_than_one = tf.cast(tf.less(diff, 1.0), \"float32\")\n",
    "    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n",
    "    return loss\n",
    "def box_loss_graph(h_fc_logits,Y_class_ids,h_fc_bbox,Y_gt_boxes):\n",
    "    print('box_loss_graph')\n",
    "    logits=tf.reshape(h_fc_logits[:,-1,:,:],(batch_size,num_boxes,n_classes))\n",
    "    correct_prediction=tf.cast(tf.equal(tf.argmax(logits,2),Y_class_ids),tf.float32)\n",
    "    #print(sess.run(correct_prediction))\n",
    "    pred_bbox=tf.reshape(h_fc_bbox[:,-1,:,:],(batch_size,num_boxes,4))\n",
    "    #print(sess.run(pred_bbox))\n",
    "    _box_loss=smooth_l1_loss(y_true=Y_gt_boxes, y_pred=pred_bbox)\n",
    "    #print(sess.run(_box_loss))\n",
    "    correct_box_loss=tf.math.multiply(tf.reshape(correct_prediction,(3,5,1)),_box_loss)\n",
    "    #print(sess.run(correct_box_loss))\n",
    "    box_loss=tf.reduce_mean(correct_box_loss)\n",
    "    #print(sess.run(box_loss))\n",
    "    #print(sess.run(tf.argmax(logits,2)))\n",
    "    return box_loss\n",
    "#sess=tf.Session()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "#box_loss=box_loss_graph(h_fc_logits,Y_class_ids,h_fc_bbox,Y_gt_boxes)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess=tf.Session()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "def smooth_loss_graph(h_fc_bbox):\n",
    "    print('smooth_loss_graph')\n",
    "    smooth_loss=0\n",
    "    for i in range(batch_size):\n",
    "        #_h_fc_1=h_fc_1[i]\n",
    "        for j in range(timesteps_size -1):\n",
    "            #print('smooth_loss_graph:{}/{}'.format(i,j))\n",
    "            smooth_loss+=tf.reduce_mean(smooth_l1_loss(h_fc_bbox[i][j],h_fc_bbox[i][j+1]))\n",
    "            #print(sess.run(smooth_loss))\n",
    "    smooth_loss=smooth_loss/(batch_size*(timesteps_size -1))\n",
    "    return smooth_loss\n",
    "#print(sess.run(smooth_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_loss(h_fc_2):\n",
    "    print('association_loss')\n",
    "    asso_loss=0\n",
    "    for i in range(batch_size-1):\n",
    "            _asso_loss=tf.reduce_sum(tf.math.multiply(h_fc_2[i],h_fc_2[i+1]),-1)\n",
    "            #print(sess.run(_asso_loss))\n",
    "            asso_loss+=tf.reduce_mean(_asso_loss)\n",
    "    asso_loss=asso_loss/(batch_size-1)\n",
    "    return asso_loss\n",
    "#sess=tf.Session()\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "#asso_loss=association_loss(h_fc_2)\n",
    "#sess.run(asso_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start lstm\n",
      "fc_bbox completed\n",
      "fc_bbox completed\n",
      "fc_bbox completed\n",
      "fc_class_logits completed\n",
      "fc_class_logits completed\n",
      "fc_class_logits completed\n",
      "fc_2 completed\n",
      "fc_2 completed\n",
      "fc_2 completed\n",
      "start computing class_loss_graph\n",
      "box_loss_graph\n",
      "smooth_loss_graph\n",
      "association_loss\n",
      "train cost=108.487427,class cost=1.550079,box cost=0.516131,asso cost=200.914612,smooth cost=0.013598\n",
      "train cost=202.994415,class cost=1.922388,box cost=0.368404,asso cost=23.215086,smooth cost=0.013607\n",
      "train cost=25.519485,class cost=1.709337,box cost=0.455378,asso cost=4.128832,smooth cost=0.013136\n",
      "train cost=6.306683,class cost=1.494356,box cost=0.480459,asso cost=0.226131,smooth cost=0.012988\n",
      "train cost=2.213934,class cost=1.341630,box cost=0.504527,asso cost=0.067405,smooth cost=0.013206\n",
      "train cost=1.926767,class cost=1.208666,box cost=0.525792,asso cost=0.091544,smooth cost=0.013687\n",
      "train cost=1.839689,class cost=1.088366,box cost=0.540962,asso cost=0.000000,smooth cost=0.014298\n",
      "train cost=1.643626,class cost=0.973088,box cost=0.547136,asso cost=0.094777,smooth cost=0.014759\n",
      "train cost=1.629760,class cost=0.856414,box cost=0.544702,asso cost=0.000000,smooth cost=0.014981\n",
      "train cost=1.416098,class cost=0.749379,box cost=0.533799,asso cost=0.000000,smooth cost=0.015015\n"
     ]
    }
   ],
   "source": [
    "print('start lstm')\n",
    "lstm_cell_1 = tf.nn.rnn_cell.LSTMCell(hidden_nodes1, state_is_tuple=True)\n",
    "lstm_cell_2 = tf.nn.rnn_cell.LSTMCell(hidden_nodes2, state_is_tuple=True)\n",
    "mul_lstm_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell_1,lstm_cell_2], state_is_tuple = True)\n",
    "init_state = mul_lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "outputs, state = tf.nn.dynamic_rnn(mul_lstm_cell, inputs=X, initial_state=init_state, time_major=False)\n",
    "h_out = outputs\n",
    "h_state = state[-1][-1]\n",
    "#print(h_out.shape)\n",
    "#print(tf.reshape(h_state[0],(1,-1)))\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "#print(sess.run(h_state))\n",
    "\n",
    "h_fc_bbox=[]\n",
    "with tf.variable_scope('fc_bbox') as scope:\n",
    "    for i in range(batch_size):\n",
    "        try:            \n",
    "            #var=tf.get_variable('fc_1/weight1')\n",
    "            _h_fc_bbox = fc_bbox(h_out[i])\n",
    "            #tf.stack([h_fc_1,_h_fc_1 ],axis=0)\n",
    "        except ValueError:\n",
    "            scope.reuse_variables()\n",
    "            _h_fc_bbox = fc_bbox(h_out[i])\n",
    "        h_fc_bbox.append(_h_fc_bbox)\n",
    "#print(h_fc_bbox)\n",
    "h_fc_bbox=tf.stack(h_fc_bbox,axis=0)\n",
    "#print(h_fc_bbox)\n",
    "h_fc_bbox=tf.reshape(h_fc_bbox,(3,30,5,-1))\n",
    "#print(h_fc_bbox)\n",
    "#print(h_fc_1)\n",
    "\n",
    "h_fc_logits=[]\n",
    "with tf.variable_scope('fc_logits') as scope:\n",
    "    for i in range(batch_size):\n",
    "        try:            \n",
    "            #var=tf.get_variable('fc_1/weight1')\n",
    "            _h_fc_logits = fc_class_logits(h_out[i])\n",
    "            #tf.stack([h_fc_1,_h_fc_1 ],axis=0)\n",
    "        except ValueError:\n",
    "            scope.reuse_variables()\n",
    "            _h_fc_logits = fc_class_logits(h_out[i])\n",
    "            #h_fc_1=tf.stack([h_fc_1,_h_fc_1 ],axis=0)\n",
    "        h_fc_logits.append(_h_fc_logits)\n",
    "        #print(h_out[i])\n",
    "        #_output=fc_1(h_out[i])\n",
    "        #_h_fc_1 = tf.nn.relu(tf.nn.xw_plus_b(h_out[i], w1, b1), name='relu')\n",
    "#print(h_fc_logits)\n",
    "h_fc_logits=tf.stack(h_fc_logits,axis=0)\n",
    "#print(h_fc_logits)\n",
    "h_fc_logits=tf.reshape(h_fc_logits,(3,30,5,-1))\n",
    "#print(h_fc_logits)\n",
    "\n",
    "h_fc_2=[]\n",
    "with tf.variable_scope('fc_2') as scope:\n",
    "    for i in range(batch_size):\n",
    "        try:            \n",
    "            _h_fc_2 = fc_2(tf.reshape(h_state[i],(1,-1)))\n",
    "            #tf.stack([h_fc_2,_h_fc_2 ],axis=-1)\n",
    "        except ValueError:\n",
    "            scope.reuse_variables()\n",
    "            _h_fc_2 = fc_2(tf.reshape(h_state[i],(1,-1)))\n",
    "            #print(_h_fc_2)\n",
    "        h_fc_2.append(_h_fc_2 )\n",
    "            #print(h_fc_2)\n",
    "#sess.run(tf.global_variables_initializer())\n",
    "#print(sess.run(h_fc_2[0]))\n",
    "h_fc_2=tf.stack(h_fc_2,axis=0)\n",
    "#print(h_fc_2)\n",
    "#h_fc_2=tf.reshape(h_fc_2,(3,5,-1))\n",
    "#print(h_fc_2)\n",
    "\n",
    "class_loss=tf.reduce_mean(class_loss_graph(h_fc_logits,Y_class_ids))\n",
    "box_loss=box_loss_graph(h_fc_logits,Y_class_ids,h_fc_bbox,Y_gt_boxes)\n",
    "smooth_loss=smooth_loss_graph(h_fc_bbox)\n",
    "asso_loss=association_loss(h_fc_2)\n",
    "total_loss=class_loss + box_loss +smooth_loss+ asso_loss\n",
    "#print(sess.run(total_loss))\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(y_input,1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "is_train=True\n",
    "save_path='/Users/stephaniexia/Documents/AI tutorial/tensorflow tutorial/'\n",
    "saver=tf.train.Saver(max_to_keep=1)\n",
    "if is_train:\n",
    "    max_acc=0\n",
    "    f=open(save_path+'acc.text','w')\n",
    "    for i in range(10):\n",
    "        cost, _ = sess.run([total_loss, train])\n",
    "        cls=sess.run(class_loss)\n",
    "        box=sess.run(box_loss)\n",
    "        asso=sess.run(asso_loss)\n",
    "        smooth=sess.run(smooth_loss)\n",
    "        print(\"train cost={:.6f},class cost={:.6f},box cost={:.6f},asso cost={:.6f},smooth cost={:.6f}\".format(cost, cls, box,asso,smooth))\n",
    "    saver.save(sess,save_path+'weights.ckpt',global_step=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def smooth_l1_loss(y_true, y_pred):\n",
    "    diff = tf.math.abs(y_true - y_pred)\n",
    "    less_than_one = tf.cast(tf.less(diff, 1.0), \"float32\")\n",
    "    loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n",
    "    return loss\n",
    "def smooth_loss(h_fc_bbox):\n",
    "    smooth_loss=0\n",
    "    for i in range(batch_size):\n",
    "        #_h_fc_bbox=h_fc_bbox[i]\n",
    "        for j in range(timesteps_size -1):\n",
    "            for bbox in range(num_boxes):\n",
    "                bbox_1=h_fc_bbox[i][j][box_num*4:(box_num+1)*4]\n",
    "                print(bbox_1)\n",
    "                bbox_2=h_fc_bbox[i][j+1][box_num*4:(box_num+1)*4]\n",
    "                print(bbox_2)\n",
    "                smooth_loss+=tf.reduce_mean(tf.square(_h_fc_1[j][i]-_h_fc_1[j+1][i]))\n",
    "\n",
    "            \n",
    "    loss = K.switch(tf.size(target_bbox) > 0,\n",
    "                    smooth_l1_loss(y_true=target_bbox, y_pred=pred_bbox),\n",
    "                    tf.constant(0.0))\n",
    "    loss = K.mean(loss)\n",
    "    loss = K.reshape(loss, [1, 1])\n",
    "    return loss\n",
    "\n",
    "    return smooth_loss\n",
    "def prediciton_loss(h_fc_1,Y):\n",
    "    prediction_loss=0\n",
    "    for i in range(batch_size):\n",
    "        #print(Y[i])\n",
    "        #print(sess.run(h_fc_1[i][-1]))\n",
    "        cross_entropy = tf.reduce_mean(Y[i] *h_fc_1[i][-1])\n",
    "        #print(sess.run(cross_entropy))\n",
    "        prediction_loss+=cross_entropy\n",
    "    return prediction_loss\n",
    "def association_loss(h_fc_2):\n",
    "    asso_loss=0\n",
    "    for i in range(batch_size-1):\n",
    "            _asso_loss=tf.reduce_mean(h_fc_2[i]*h_fc_2[i+1])\n",
    "            #print(sess.run(_asso_loss))\n",
    "            asso_loss+=_asso_loss\n",
    "    return asso_loss\n",
    "smooth_loss=smooth_loss(h_fc_1)\n",
    "prediction_loss=prediciton_loss(h_fc_1,Y)\n",
    "association_loss=association_loss(h_fc_2)\n",
    "print(sess.run(smooth_loss))\n",
    "print(sess.run(prediction_loss))\n",
    "print(sess.run(association_loss))\n",
    "total_loss=smooth_loss+prediction_loss+association_loss\n",
    "print(sess.run(total_loss))\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "#correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(y_input,1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(100):\n",
    "    cost, _ = sess.run([total_loss, train])\n",
    "    print(\"train cost={:.6f}\".format(cost))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
